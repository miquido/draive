{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic data extraction\n",
    "\n",
    "One of the most useful abilities of LLMs is to extract the information from an unstructured source and put it into a desired structure. The typical flow would be to load some kind of a document and instruct the model to provide a json with a list of required fields. Draive comes with a dedicated solution for generating structured output which we can use to extract required information from any source. We will use OpenAI for this task, make sure to provide .env file with `OPENAI_API_KEY` key before running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draive import load_env\n",
    "\n",
    "load_env() # load .env variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data source\n",
    "\n",
    "For our data source we will use a local file with plain text contents. For more advanced use and more data formats you might need to perform additional steps like the text extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load document contents for the extraction\n",
    "document: str\n",
    "with open(\"data/personal_document.txt\") as file:\n",
    "    document = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structure\n",
    "\n",
    "`DataModel` base class is a perfect tool to define required data structure. We will use it to define a simple structure for this example. Please see AdvancedState.ipynb for more details about the advanced data model customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draive import DataModel\n",
    "\n",
    "\n",
    "class PersonalData(DataModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    age: int | None = None\n",
    "    country: str | None = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction\n",
    "\n",
    "Since we are going to generate a structured output we will use the `generate_model` function which automatically decodes the result of LLM call into a python object. We have to prepare the execution context, provide an instruction, source document and specify the data model and we can expect extracted data to be available. Keep in mind that depending on the task it might be beneficial to prepare a specific instruction and/or specify more details about the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_name: John\n",
      "last_name: Doe\n",
      "age: 21\n",
      "country: Canada\n"
     ]
    }
   ],
   "source": [
    "from draive import ctx, generate_model\n",
    "from draive.openai import OpenAIChatConfig, openai_lmm\n",
    "\n",
    "async with ctx.scope(\n",
    "    \"data_extraction\",\n",
    "    # define used LMM to be OpenAI within the context\n",
    "    openai_lmm(),\n",
    "    OpenAIChatConfig(model=\"gpt-4o-mini\")\n",
    "):\n",
    "    result: PersonalData = await generate_model(\n",
    "        # define model to generate\n",
    "        PersonalData,\n",
    "        # provide additional instructions\n",
    "        # note that the data structure will be automatically explained to LLM\n",
    "        instruction=\"Please extract information from the given input\",\n",
    "        # we will provide the document as an input\n",
    "        input=document,\n",
    "    )\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customization\n",
    "\n",
    "We can customize data extraction by specifying more details about the model itself i.e. description of some fields. We can also choose to take more control over the prompt used to generate the model. Instead of relying on the automatically generated model description we can specify it manually or provide a placeholder inside the instruction to put it in specific place. Additionally we can choose between full json schema description and the simplified description which works better with smaller, less capable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON schema:\n",
      "{\n",
      "  \"type\": \"object\",\n",
      "  \"properties\": {\n",
      "    \"first_name\": {\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"last_name\": {\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"age\": {\n",
      "      \"oneOf\": [\n",
      "        {\n",
      "          \"type\": \"integer\"\n",
      "        },\n",
      "        {\n",
      "          \"type\": \"null\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"country\": {\n",
      "      \"oneOf\": [\n",
      "        {\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        {\n",
      "          \"type\": \"null\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"first_name\",\n",
      "    \"last_name\"\n",
      "  ]\n",
      "}\n",
      "Simplified schema:\n",
      "{\n",
      "  \"first_name\": \"string\",\n",
      "  \"last_name\": \"string\",\n",
      "  \"age\": \"integer|null\",\n",
      "  \"country\": \"string|null\"\n",
      "}\n",
      "Result:\n",
      "first_name: John\n",
      "last_name: Doe\n",
      "age: 21\n",
      "country: Canada\n"
     ]
    }
   ],
   "source": [
    "from draive import ctx, generate_model\n",
    "\n",
    "async with ctx.scope(\n",
    "    \"customized_extraction\",\n",
    "    # define used LMM to be OpenAI within the context\n",
    "    openai_lmm(),\n",
    "    OpenAIChatConfig(model=\"gpt-4o-mini\")\n",
    "):\n",
    "    result: PersonalData = await generate_model(\n",
    "        PersonalData,\n",
    "        instruction=(\n",
    "            # provide extended instructions and take full control over the prompt\n",
    "            \"Please extract information from the given input.\"\n",
    "            \" Put it into the JSON according to the following description:\\n\"\n",
    "            \"{schema}\"  # 'schema' is the name of the format argument used to fill in the schema\n",
    "        ),\n",
    "        input=document,\n",
    "        # we will use the simplified schema description\n",
    "        # you can also skip adding schema at all\n",
    "        schema_injection=\"simplified\",\n",
    "    )\n",
    "\n",
    "    print(f\"JSON schema:\\n{PersonalData.json_schema(indent=2)}\")\n",
    "    print(f\"Simplified schema:\\n{PersonalData.simplified_schema(indent=2)}\")\n",
    "    print(f\"Result:\\n{result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
