{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RAG\n",
    "\n",
    "RAG is a technique of adding additional information to the LLM context to achieve better results. This applies to adding a knowledge source to be used by LLM to generate correct response as well as to introducing details about the task or personalization for more tailored result. The typical flow would be to prepare a vector index and search through the data to extend the model input with suitable context. We will use OpenAI services for this task, make sure to provide .env file with `OPENAI_API_KEY` key before running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draive import load_env\n",
    "\n",
    "load_env() # load .env variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "We are going to use a local file with plain text as out input. After loading the file content we will prepare it for embedding by splitting the original text to smaller chunks. This allows to fit the data into limited context windows and to additionally filter out unnecessary information. We are going to use a basic splitter looking for paragraphs structure defined by multiple subsequent newlines in the text. Chunks will be put into a structured envelope which allows to store additional information such as a full text of the document or additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 11 chunks:\n",
      "---\n",
      "John Doe, originally from a small farm in Texas, has been living in Vancouver for more than three years. His fascination with Canada began when he first visited the country at the age of seven. The experience left a lasting impression on him, and he knew that one day he would make Canada his home.\n",
      "---\n",
      "At the age of 18, John made the bold decision to leave his rural life in Texas behind and move to Vancouver. The transition was not without its challenges, as he had to adapt to the fast-paced city life, which was a stark contrast to the slow, quiet days on the\n",
      "---\n",
      "stark contrast to the slow, quiet days on the farm. However, John's determination and love for his new home helped him overcome any obstacles he faced.\n",
      "---\n",
      "Now, at 21, John has fully embraced his life in Vancouver. He has made new friends, discovered his favorite local spots, and even started attending college to pursue his passion for environmental science. The city's stunning natural beauty, with its lush forests and\n",
      "---\n",
      "natural beauty, with its lush forests and pristine coastline, reminds him of why he fell in love with Canada in the first place.\n",
      "---\n",
      "John's days are filled with exploring the city's diverse neighborhoods, trying new cuisines, and participating in various outdoor activities. He has become an avid hiker, taking advantage of the numerous trails in and around Vancouver. On weekends,\n",
      "---\n",
      "in and around Vancouver. On weekends, he often finds himself hiking in the nearby mountains, breathing in the crisp air and marveling at the breathtaking views.\n",
      "---\n",
      "Despite the occasional homesickness for his family and the familiarity of his Texas farm, John knows that Vancouver is where he belongs. The city has captured his heart, and he can't imagine living anywhere else. He dreams of one day working in the field of environmental\n",
      "---\n",
      "of one day working in the field of environmental conservation, helping to protect the natural wonders that made him fall in love with Canada.\n",
      "---\n",
      "As John reflects on his journey from a small farm in Texas to the vibrant city of Vancouver, he feels a sense of pride and accomplishment. He knows that his seven-year-old self would be proud of the life he has built in the country that captured his\n",
      "---\n",
      "life he has built in the country that captured his imagination all those years ago. With a smile on his face, John looks forward to the future and all the adventures that Vancouver has in store for him.\n"
     ]
    }
   ],
   "source": [
    "from draive import DataModel, Tokenization, count_text_tokens, ctx, split_text\n",
    "from draive.openai import openai_tokenize_text\n",
    "\n",
    "\n",
    "# define chunk data structure\n",
    "class DocumentChunk(DataModel):\n",
    "    full_document: str\n",
    "    content: str\n",
    "\n",
    "# prepare data chunks\n",
    "document_chunks: list[DocumentChunk]\n",
    "async with ctx.new(\n",
    "    state=[  # define tokenizer for this context\n",
    "        Tokenization(tokenize_text=openai_tokenize_text),\n",
    "    ],\n",
    "):\n",
    "    # load the document contents\n",
    "    with open(\"data/personal_document.txt\") as file:\n",
    "        document = file.read()\n",
    "\n",
    "        document_chunks = [\n",
    "            DocumentChunk(\n",
    "                full_document=document,\n",
    "                content=chunk,\n",
    "            )\n",
    "            # split document text into smaller parts\n",
    "            for chunk in split_text(\n",
    "                text=document,\n",
    "                separators=(\"\\n\\n\", \" \"),\n",
    "                part_size=64,\n",
    "                count_size=count_text_tokens,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "print(f\"Prepared {len(document_chunks)} chunks:\\n---\")\n",
    "print(\"\\n---\\n\".join(chunk.content for chunk in document_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data indexing\n",
    "\n",
    "When we have prepared the data, we can now use a vector index to make it searchable. We are going to use in-memory, volatile vector index which can be useful for a quick search. Preparing the index requires defining the text embedding method. In this example we are going to use OpenAI embedding solution. After defining all required parameters and providers we can prepare the index using our data. In order to ensure proper data embedding it is required to specify what value will be used to prepare the vector. In this case we specify the chunk content to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draive import TextEmbedding, VolatileVectorIndex\n",
    "from draive.openai import OpenAIClient, openai_embed_text\n",
    "\n",
    "# prepare vector index\n",
    "vector_index: VolatileVectorIndex\n",
    "async with ctx.new(\n",
    "    state=[  # define embedding provider for this context\n",
    "        TextEmbedding(embed=openai_embed_text),\n",
    "        Tokenization(tokenize_text=openai_tokenize_text),\n",
    "    ],  # provide OpenAI services client\n",
    "    dependencies=[OpenAIClient],\n",
    "):\n",
    "    vector_index = VolatileVectorIndex()\n",
    "    # add document chunks to the index\n",
    "    await vector_index.index(\n",
    "        DocumentChunk,\n",
    "        values=document_chunks,\n",
    "        # define what value will be embedded for each chunk\n",
    "        indexed_value=DocumentChunk._.content,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching the index\n",
    "\n",
    "Now we have everything set up to do our search. Instead of searching directly, we are going to leverage the ability of LLMs to call tools. We have to define a search tool for that task, which will use our index to deliver the results. LLM will make use of this tool to look for the answer based on the search result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Doe is currently living in Vancouver.\n"
     ]
    }
   ],
   "source": [
    "from draive import LMM, Toolbox, generate_text, tool\n",
    "from draive.openai import openai_lmm_invocation\n",
    "\n",
    "\n",
    "@tool(name=\"search\") # prepare a simple tool for searching the index\n",
    "async def index_search_tool(query: str) -> str:\n",
    "    results: list[DocumentChunk] = await vector_index.search(\n",
    "        DocumentChunk,\n",
    "        query=query,\n",
    "        limit=3,\n",
    "    )\n",
    "\n",
    "    return \"\\n---\\n\".join(result.content for result in results)\n",
    "\n",
    "\n",
    "async with ctx.new(\n",
    "    state=[  # define used dependencies and services\n",
    "        TextEmbedding(embed=openai_embed_text),\n",
    "        Tokenization(tokenize_text=openai_tokenize_text),\n",
    "        LMM(invocation=openai_lmm_invocation),\n",
    "    ],\n",
    "    dependencies=[OpenAIClient],\n",
    "):\n",
    "    # use the tool to augment LLM generation by suitable document parts\n",
    "    result: str = await generate_text(\n",
    "        instruction=\"Answer the questions based on provided data\",\n",
    "        input=\"Where is John Doe living?\",\n",
    "        # suggest the tool to ensure its usage\n",
    "        tools=Toolbox(suggest=index_search_tool),\n",
    "    )\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
