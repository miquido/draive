{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic MCP\n",
    "\n",
    "ModelContextProtocol (MCP) allows to easily extend application and LLM capabilities using standardized feature implementation. draive library comes with support for MCP both as a server and client allowing to build LLM based application even faster with more code reuse. Lets have a small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/Jan/2025:17:14:39 +0000 [DEBUG] [mcp] [86261f3e9c354cf1bc0cf4ef2101076a] [mcp] [d238c2ce05264b3fa7c721fdfe0d201f] Entering context...\n",
      "15/Jan/2025:17:14:40 +0000 [DEBUG] [conversation_completion] [86261f3e9c354cf1bc0cf4ef2101076a] [conversation_completion] [d3668c8ed9d2464cab020a3f3f0b6020] Entering context...\n",
      "15/Jan/2025:17:14:40 +0000 [DEBUG] [openai_lmm_invocation] [86261f3e9c354cf1bc0cf4ef2101076a] [openai_lmm_invocation] [69c8fbb15c26420292d3ed126c6919bd] Entering context...\n",
      "15/Jan/2025:17:14:40 +0000 [DEBUG] [httpcore.connection] connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "15/Jan/2025:17:14:40 +0000 [DEBUG] [httpcore.connection] connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x14a258b00>\n",
      "15/Jan/2025:17:14:40 +0000 [DEBUG] [httpcore.connection] start_tls.started ssl_context=<ssl.SSLContext object at 0x149c7c4d0> server_hostname='api.openai.com' timeout=5.0\n",
      "15/Jan/2025:17:14:40 +0000 [DEBUG] [httpcore.connection] start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x14a21f1a0>\n",
      "15/Jan/2025:17:14:40 +0000 [DEBUG] [httpcore.http11] send_request_headers.started request=<Request [b'POST']>\n",
      "15/Jan/2025:17:14:40 +0000 [DEBUG] [httpcore.http11] send_request_headers.complete\n",
      "15/Jan/2025:17:14:40 +0000 [DEBUG] [httpcore.http11] send_request_body.started request=<Request [b'POST']>\n",
      "15/Jan/2025:17:14:40 +0000 [DEBUG] [httpcore.http11] send_request_body.complete\n",
      "15/Jan/2025:17:14:40 +0000 [DEBUG] [httpcore.http11] receive_response_headers.started request=<Request [b'POST']>\n",
      "15/Jan/2025:17:14:41 +0000 [DEBUG] [httpcore.http11] receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 15 Jan 2025 16:14:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'miquido-gpt'), (b'openai-processing-ms', b'578'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9999942'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_c3b1b7b2ce3bd0db1e90edb4a14623b2'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=HaEQj.Zmg6.LqwdMfRyLeFJziC9MSQt8JgzXEAYypGY-1736957681-1.0.1.1-.8fh9M8lscyIlgWj345nH.xjnxEh0sBky340f.ZA7s3idt6GoDx8QfWJpxiWccANOEfIpGVFv9AYIjRxLqT2uA; path=/; expires=Wed, 15-Jan-25 16:44:41 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=ZMmO7VnZMk9jJ._G66dKPOTl6is_EGomUyVAC5E.MOk-1736957681860-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'902728fffc50b161-WAW'), (b'Content-Encoding', b'gzip')])\n",
      "15/Jan/2025:17:14:41 +0000 [DEBUG] [httpcore.http11] receive_response_body.started request=<Request [b'POST']>\n",
      "15/Jan/2025:17:14:41 +0000 [DEBUG] [httpcore.http11] receive_response_body.complete\n",
      "15/Jan/2025:17:14:41 +0000 [DEBUG] [httpcore.http11] response_closed.started\n",
      "15/Jan/2025:17:14:41 +0000 [DEBUG] [httpcore.http11] response_closed.complete\n",
      "15/Jan/2025:17:14:41 +0000 [DEBUG] [openai_lmm_invocation] [86261f3e9c354cf1bc0cf4ef2101076a] [openai_lmm_invocation] [69c8fbb15c26420292d3ed126c6919bd] ...exiting context after 1.30s\n",
      "15/Jan/2025:17:14:41 +0000 [DEBUG] [conversation_completion] [86261f3e9c354cf1bc0cf4ef2101076a] [conversation_completion] [d3668c8ed9d2464cab020a3f3f0b6020] Received conversation tool calls\n",
      "15/Jan/2025:17:14:41 +0000 [DEBUG] [list_directory] [86261f3e9c354cf1bc0cf4ef2101076a] [list_directory] [37cc4b7bd4614df98cfab2bc608ee037] Entering context...\n",
      "15/Jan/2025:17:14:41 +0000 [DEBUG] [list_directory] [86261f3e9c354cf1bc0cf4ef2101076a] [list_directory] [37cc4b7bd4614df98cfab2bc608ee037] ...exiting context after 0.00s\n",
      "15/Jan/2025:17:14:41 +0000 [DEBUG] [openai_lmm_invocation] [86261f3e9c354cf1bc0cf4ef2101076a] [openai_lmm_invocation] [7ccbb8f6dfc6400e85034c9ecd580063] Entering context...\n",
      "15/Jan/2025:17:14:41 +0000 [DEBUG] [httpcore.http11] send_request_headers.started request=<Request [b'POST']>\n",
      "15/Jan/2025:17:14:41 +0000 [DEBUG] [httpcore.http11] send_request_headers.complete\n",
      "15/Jan/2025:17:14:41 +0000 [DEBUG] [httpcore.http11] send_request_body.started request=<Request [b'POST']>\n",
      "15/Jan/2025:17:14:41 +0000 [DEBUG] [httpcore.http11] send_request_body.complete\n",
      "15/Jan/2025:17:14:41 +0000 [DEBUG] [httpcore.http11] receive_response_headers.started request=<Request [b'POST']>\n",
      "15/Jan/2025:17:14:43 +0000 [DEBUG] [httpcore.http11] receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 15 Jan 2025 16:14:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'miquido-gpt'), (b'openai-processing-ms', b'1272'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9999907'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_157bdd5e2958663ef439a579ccce07ef'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'90272907dd85b161-WAW'), (b'Content-Encoding', b'gzip')])\n",
      "15/Jan/2025:17:14:43 +0000 [DEBUG] [httpcore.http11] receive_response_body.started request=<Request [b'POST']>\n",
      "15/Jan/2025:17:14:43 +0000 [DEBUG] [httpcore.http11] receive_response_body.complete\n",
      "15/Jan/2025:17:14:43 +0000 [DEBUG] [httpcore.http11] response_closed.started\n",
      "15/Jan/2025:17:14:43 +0000 [DEBUG] [httpcore.http11] response_closed.complete\n",
      "15/Jan/2025:17:14:43 +0000 [DEBUG] [openai_lmm_invocation] [86261f3e9c354cf1bc0cf4ef2101076a] [openai_lmm_invocation] [7ccbb8f6dfc6400e85034c9ecd580063] ...exiting context after 1.44s\n",
      "15/Jan/2025:17:14:43 +0000 [DEBUG] [conversation_completion] [86261f3e9c354cf1bc0cf4ef2101076a] [conversation_completion] [d3668c8ed9d2464cab020a3f3f0b6020] Received conversation result\n",
      "15/Jan/2025:17:14:43 +0000 [DEBUG] [conversation_completion] [86261f3e9c354cf1bc0cf4ef2101076a] [conversation_completion] [d3668c8ed9d2464cab020a3f3f0b6020] ...exiting context after 2.75s\n",
      "Your desktop contains the following files:\n",
      "\n",
      "1. `.DS_Store`\n",
      "2. `.localized`\n",
      "3. `ClaudePrompt.md`\n",
      "4. `gemini-for-google-workspace-prompting-guide-101.pdf`\n",
      "5. `panda.png`\n",
      "15/Jan/2025:17:14:43 +0000 [DEBUG] [mcp] [86261f3e9c354cf1bc0cf4ef2101076a] [mcp] [d238c2ce05264b3fa7c721fdfe0d201f] ...exiting context after 3.71s\n"
     ]
    }
   ],
   "source": [
    "from draive import (\n",
    "    ConversationMessage,\n",
    "    Toolbox,\n",
    "    conversation_completion,\n",
    "    ctx,\n",
    "    load_env,\n",
    "    setup_logging,\n",
    ")\n",
    "from draive.mcp import MCPClient\n",
    "from draive.openai import OpenAIChatConfig, openai_lmm\n",
    "\n",
    "load_env() # load .env variables\n",
    "setup_logging(\"mcp\")\n",
    "\n",
    "\n",
    "# initialize dependencies and configuration\n",
    "async with ctx.scope(\n",
    "    \"mcp\",\n",
    "    openai_lmm(),  # define used LMM to use OpenAI\n",
    "    OpenAIChatConfig(model=\"gpt-4o-mini\"),  # configure OpenAI model\n",
    "    # prepare MCPClient, it will handle connection lifetime through context\n",
    "    # and provide associated state with MCP functionalities\n",
    "    disposables=[\n",
    "        # we are going to use stdio connection with one of the example servers\n",
    "        MCPClient.stdio(\n",
    "            command=\"npx\",\n",
    "            args=[\n",
    "                \"-y\",\n",
    "                \"@modelcontextprotocol/server-filesystem\",\n",
    "                \"/Users/miquido/Desktop\",\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    "):\n",
    "    # request model using any appropriate method, i.e. conversation for chat\n",
    "    response: ConversationMessage = await conversation_completion(\n",
    "        # provide a prompt instruction\n",
    "        instruction=\"You can access files on behalf of the user on their machine using available tools.\"\n",
    "        \" Desktop directory path is `/Users/miquido/Desktop`\",\n",
    "        # add user input\n",
    "        input=\"What is on my desktop?\",\n",
    "        # define tools available to the model from MCP extensions\n",
    "        tools=await Toolbox.external(),\n",
    "    )\n",
    "    print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
