{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic MCP\n",
    "\n",
    "ModelContextProtocol (MCP) allows to easily extend application and LLM capabilities using standardized feature implementation. draive library comes with support for MCP both as a server and client allowing to build LLM based application even faster with more code reuse. Lets have a small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/Jan/2025:16:46:24 +0000 [DEBUG] [mcp] [1d73548024914b4d859339c59e2fc8a5] [mcp] [72f5df3949164e7fa66c1460cdbb9ab2] Entering context...\n",
      "14/Jan/2025:16:46:24 +0000 [DEBUG] [conversation_completion] [1d73548024914b4d859339c59e2fc8a5] [conversation_completion] [30301b655cbe41bcb69f03b33b463a77] Entering context...\n",
      "14/Jan/2025:16:46:24 +0000 [DEBUG] [openai_lmm_invocation] [1d73548024914b4d859339c59e2fc8a5] [openai_lmm_invocation] [063d4f2b6cb24fc4a5955aff6bbdb828] Entering context...\n",
      "14/Jan/2025:16:46:24 +0000 [DEBUG] [httpcore.connection] connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "14/Jan/2025:16:46:24 +0000 [DEBUG] [httpcore.connection] connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x119b52600>\n",
      "14/Jan/2025:16:46:24 +0000 [DEBUG] [httpcore.connection] start_tls.started ssl_context=<ssl.SSLContext object at 0x119f29150> server_hostname='api.openai.com' timeout=5.0\n",
      "14/Jan/2025:16:46:24 +0000 [DEBUG] [httpcore.connection] start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x119cab500>\n",
      "14/Jan/2025:16:46:24 +0000 [DEBUG] [httpcore.http11] send_request_headers.started request=<Request [b'POST']>\n",
      "14/Jan/2025:16:46:24 +0000 [DEBUG] [httpcore.http11] send_request_headers.complete\n",
      "14/Jan/2025:16:46:24 +0000 [DEBUG] [httpcore.http11] send_request_body.started request=<Request [b'POST']>\n",
      "14/Jan/2025:16:46:24 +0000 [DEBUG] [httpcore.http11] send_request_body.complete\n",
      "14/Jan/2025:16:46:24 +0000 [DEBUG] [httpcore.http11] receive_response_headers.started request=<Request [b'POST']>\n",
      "14/Jan/2025:16:46:25 +0000 [DEBUG] [httpcore.http11] receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 14 Jan 2025 15:46:25 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'miquido-gpt'), (b'openai-processing-ms', b'661'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9999942'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_975ee280d44e4dd0cd708c67fd5b1ccc'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=mC3H5K4GLmMdbILfJQm28j397xtUPSmQsqTK0PTyNv8-1736869585-1.0.1.1-Hxkbxj6HNMyJwycym_YT0fme_LY.tOjtxjJrdnRn1serMv_XxEopbUjYy0TY19t1I.XPsPv_e4BLfQR85gruaw; path=/; expires=Tue, 14-Jan-25 16:16:25 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=L6FjUl7eMqSh.P4Y3UT6H9wEPf1pYntbDxq8YvKPraE-1736869585644-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'901ec2392e6a020f-WAW'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "14/Jan/2025:16:46:25 +0000 [DEBUG] [httpcore.http11] receive_response_body.started request=<Request [b'POST']>\n",
      "14/Jan/2025:16:46:25 +0000 [DEBUG] [httpcore.http11] receive_response_body.complete\n",
      "14/Jan/2025:16:46:25 +0000 [DEBUG] [httpcore.http11] response_closed.started\n",
      "14/Jan/2025:16:46:25 +0000 [DEBUG] [httpcore.http11] response_closed.complete\n",
      "14/Jan/2025:16:46:25 +0000 [DEBUG] [openai_lmm_invocation] [1d73548024914b4d859339c59e2fc8a5] [openai_lmm_invocation] [063d4f2b6cb24fc4a5955aff6bbdb828] ...exiting context after 0.88s\n",
      "14/Jan/2025:16:46:25 +0000 [DEBUG] [conversation_completion] [1d73548024914b4d859339c59e2fc8a5] [conversation_completion] [30301b655cbe41bcb69f03b33b463a77] Received conversation tool calls\n",
      "14/Jan/2025:16:46:25 +0000 [DEBUG] [list_directory] [1d73548024914b4d859339c59e2fc8a5] [list_directory] [a8f67a04e2c648eb921ee4e16bd58db1] Entering context...\n",
      "14/Jan/2025:16:46:25 +0000 [DEBUG] [list_directory] [1d73548024914b4d859339c59e2fc8a5] [list_directory] [a8f67a04e2c648eb921ee4e16bd58db1] ...exiting context after 0.00s\n",
      "14/Jan/2025:16:46:25 +0000 [DEBUG] [openai_lmm_invocation] [1d73548024914b4d859339c59e2fc8a5] [openai_lmm_invocation] [cbe73514c39b47d1a512040760354f1f] Entering context...\n",
      "14/Jan/2025:16:46:25 +0000 [DEBUG] [httpcore.http11] send_request_headers.started request=<Request [b'POST']>\n",
      "14/Jan/2025:16:46:25 +0000 [DEBUG] [httpcore.http11] send_request_headers.complete\n",
      "14/Jan/2025:16:46:25 +0000 [DEBUG] [httpcore.http11] send_request_body.started request=<Request [b'POST']>\n",
      "14/Jan/2025:16:46:25 +0000 [DEBUG] [httpcore.http11] send_request_body.complete\n",
      "14/Jan/2025:16:46:25 +0000 [DEBUG] [httpcore.http11] receive_response_headers.started request=<Request [b'POST']>\n",
      "14/Jan/2025:16:46:27 +0000 [DEBUG] [httpcore.http11] receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 14 Jan 2025 15:46:27 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'miquido-gpt'), (b'openai-processing-ms', b'1486'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9999907'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_ff18b30a362870d97c153991d30728e9'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'901ec23e7c56020f-WAW'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "14/Jan/2025:16:46:27 +0000 [DEBUG] [httpcore.http11] receive_response_body.started request=<Request [b'POST']>\n",
      "14/Jan/2025:16:46:27 +0000 [DEBUG] [httpcore.http11] receive_response_body.complete\n",
      "14/Jan/2025:16:46:27 +0000 [DEBUG] [httpcore.http11] response_closed.started\n",
      "14/Jan/2025:16:46:27 +0000 [DEBUG] [httpcore.http11] response_closed.complete\n",
      "14/Jan/2025:16:46:27 +0000 [DEBUG] [openai_lmm_invocation] [1d73548024914b4d859339c59e2fc8a5] [openai_lmm_invocation] [cbe73514c39b47d1a512040760354f1f] ...exiting context after 1.65s\n",
      "14/Jan/2025:16:46:27 +0000 [DEBUG] [conversation_completion] [1d73548024914b4d859339c59e2fc8a5] [conversation_completion] [30301b655cbe41bcb69f03b33b463a77] Received conversation result\n",
      "14/Jan/2025:16:46:27 +0000 [DEBUG] [conversation_completion] [1d73548024914b4d859339c59e2fc8a5] [conversation_completion] [30301b655cbe41bcb69f03b33b463a77] ...exiting context after 2.54s\n",
      "Your desktop contains the following files:\n",
      "\n",
      "1. **.DS_Store** (system file)\n",
      "2. **.localized** (system file)\n",
      "3. **ClaudePrompt.md** (Markdown file)\n",
      "4. **gemini-for-google-workspace-prompting-guide-101.pdf** (PDF file)\n",
      "5. **panda.png** (Image file)\n",
      "14/Jan/2025:16:46:27 +0000 [DEBUG] [mcp] [1d73548024914b4d859339c59e2fc8a5] [mcp] [72f5df3949164e7fa66c1460cdbb9ab2] ...exiting context after 3.17s\n"
     ]
    }
   ],
   "source": [
    "from collections.abc import AsyncGenerator\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "from mcp.client.session import ClientSession\n",
    "from mcp.client.stdio import StdioServerParameters, stdio_client\n",
    "\n",
    "from draive import (\n",
    "    ConversationMessage,\n",
    "    Toolbox,\n",
    "    conversation_completion,\n",
    "    ctx,\n",
    "    load_env,\n",
    "    setup_logging,\n",
    ")\n",
    "from draive.mcp import MCPClient\n",
    "from draive.openai import OpenAIChatConfig, openai_lmm\n",
    "\n",
    "load_env() # load .env variables\n",
    "setup_logging(\"mcp\")\n",
    "\n",
    "# prepare MCP connection and session\n",
    "@asynccontextmanager\n",
    "async def mcp_session() -> AsyncGenerator[ClientSession]:\n",
    "    async with stdio_client(\n",
    "        # we are going to use stdio connection within one of the example servers\n",
    "        StdioServerParameters(\n",
    "            command=\"npx\",\n",
    "            args=[\n",
    "                \"-y\",\n",
    "                \"@modelcontextprotocol/server-filesystem\",\n",
    "                \"/Users/miquido/Desktop\",\n",
    "            ],\n",
    "        )\n",
    "    ) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            yield session\n",
    "\n",
    "\n",
    "# initialize dependencies and configuration\n",
    "async with ctx.scope(\n",
    "    \"mcp\",\n",
    "    openai_lmm(),  # define used LMM to use OpenAI\n",
    "    OpenAIChatConfig(model=\"gpt-4o-mini\"),  # configure OpenAI model\n",
    "    # prepare MCPClient, it will handle connection lifetime through context\n",
    "    # and provide associated state with MCP functionalities\n",
    "    disposables=[MCPClient(mcp_session())]\n",
    "):\n",
    "    # request model using any appropriate method, i.e. conversation for chat\n",
    "    response: ConversationMessage = await conversation_completion(\n",
    "        # provide a prompt instruction\n",
    "        instruction=\"You can access files on behalf of the user on their machine using available tools.\"\n",
    "        \" Desktop directory path is `/Users/miquido/Desktop`\",\n",
    "        # add user input\n",
    "        input=\"What is on my desktop?\",\n",
    "        # define tools available to the model from MCP extensions\n",
    "        tools=await Toolbox.external(),\n",
    "    )\n",
    "    print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
