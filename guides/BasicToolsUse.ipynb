{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tools use\n",
    "\n",
    "Various LLM use cases can utilize function/tool calls to extend LLM capabilities. Draive comes with a dedicated solution to prepare tools and control its execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool definition\n",
    "\n",
    "Let's start by defining a simple tool. Tool is usually a function which is explained to LLM and can be requested to be used. Tools can require generation of arguments and have to return some value. Tools defined within draive are python async function annotated with `tool` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draive import tool\n",
    "\n",
    "\n",
    "@tool # simply annotate function as a tool, tools can have arguments using basic types\n",
    "async def current_time(location: str) -> str:\n",
    "    # return result of tool execution, we are using fake time here\n",
    "    return f\"Time in {location} is 9:53:22\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool use\n",
    "\n",
    "After defining a tool we can use it in multiple scenarios to extend LLM capabilities. All of the tool arguments will be validated when calling. It includes type check and any additional validation if defined. You can still use it as a regular function, although, all tools have to be executed within draive context scope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time in London is 9:53:22\n"
     ]
    }
   ],
   "source": [
    "from draive import ctx\n",
    "\n",
    "async with ctx.scope(\"basics\"):\n",
    "    # we can still use it as a regular function\n",
    "    # but it has to be executed within context scope\n",
    "    print(await current_time(location=\"London\"))\n",
    "\n",
    "# await current_time(location=\"London\") # error! out of context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest benefit of defining a tool is when using LLM. We can tell the model to use available tools to extend its capabilities. Higher level interfaces automatically handle tool calls and going back with its result to LLM to receive the final result. We can see how it works within a simple text generator. Tools are provided within a `Toolbox` object which allows customizing tools execution. You can prepare a `Toolbox` each time it is used or reuse preconstructed one. We will use OpenAI GPT model as it natively supports tool use. Make sure to provide .env file with `OPENAI_API_KEY` key before running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current time in New York is 9:53 AM.\n"
     ]
    }
   ],
   "source": [
    "from draive import Toolbox, generate_text, load_env\n",
    "from draive.openai import OpenAIChatConfig, openai_lmm\n",
    "\n",
    "load_env()\n",
    "\n",
    "async with ctx.scope(\n",
    "    \"basics\",\n",
    "    # define used LMM to be OpenAI within the context\n",
    "    openai_lmm(),\n",
    "    OpenAIChatConfig(model=\"gpt-4o-mini\"),\n",
    "):\n",
    "    result: str = await generate_text(\n",
    "        instruction=\"You are a helpful assistant\",\n",
    "        input=\"What is the time in New York?\",\n",
    "        tools=[current_time],\n",
    "    )\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool details\n",
    "\n",
    "Tools can be customized and extended in various ways depending on use case. First of all we can customize tool arguments and help LLM to better understand how to use given tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'fun_fact', 'description': 'Find a fun fact in a given topic', 'parameters': {'type': 'object', 'properties': {'topic': {'type': 'string', 'description': 'Topic of a fact to find'}}, 'required': []}}\n"
     ]
    }
   ],
   "source": [
    "from draive import Argument\n",
    "\n",
    "\n",
    "@tool( # this time we will use additional arguments within tool annotation\n",
    "    # we can define an alias used as the tool name when explaining it to LLM,\n",
    "    # default name is the name of python function\n",
    "    name=\"fun_fact\",\n",
    "    # additionally we can explain the tool purpose by using description\n",
    "    description=\"Find a fun fact in a given topic\",\n",
    ")\n",
    "async def customized(\n",
    "    # we can also annotate arguments to provide even more details\n",
    "    # and specify argument handling logic\n",
    "    arg: str = Argument(\n",
    "        # we can alias each argument name\n",
    "        aliased=\"topic\",\n",
    "        # further describe its use\n",
    "        description=\"Topic of a fact to find\",\n",
    "        # provide default value or default value factory\n",
    "        default=\"random\",\n",
    "        # end more, including custom validators\n",
    "    ),\n",
    ") -> str:\n",
    "    return f\"{arg} is very funny on its own!\"\n",
    "\n",
    "# we can examine tool specification which is similar to\n",
    "# how `State` and `DataModel` specification/schema is built\n",
    "print(customized.specification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toolbox\n",
    "\n",
    "We have already used a Toolbox to ask for an extended LLM completion. However Toolbox allows us to specify some additional details regarding the tools execution like the tool calls limit or a tool suggestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The funny thing about LLMs (Large Language Models) is that they can generate text that sounds remarkably coherent and intelligent, but they often have no idea what they’re talking about! It's like asking a parrot for life advice — it might sound profound, but it’s just mimicking what it has heard. Just imagine chatting with a robot that can quote Shakespeare while simultaneously forgetting your name!\n"
     ]
    }
   ],
   "source": [
    "async with ctx.scope(\n",
    "    \"basics\",\n",
    "    # define used LMM to be OpenAI within the context\n",
    "    openai_lmm(),\n",
    "    OpenAIChatConfig(model=\"gpt-4o-mini\"),\n",
    "):\n",
    "    result: str = await generate_text(\n",
    "        instruction=\"You are a funny assistant\",\n",
    "        input=\"What is the funny thing about LLMs?\",\n",
    "        tools=Toolbox.of(\n",
    "            # we can define any number of tools within a toolbox\n",
    "            current_time,\n",
    "            # we can also force any given tool use within the first LLM call\n",
    "            suggest=customized,\n",
    "            # we can limit how many tool calls are allowed\n",
    "            # before the final result is returned\n",
    "            repeated_calls_limit=2,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "All of the tool usage is automatically traced within scope metrics. We can see the details about their execution when using a logger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/Dec/2024:17:51:57 +0000 [INFO] [basics] [185e42d863da45869acc9702d6220949] [basics] [2e773dc58ff14e37a6398552b5766a96] Started...\n",
      "19/Dec/2024:17:51:57 +0000 [INFO] [basics] [b98eb4e0d921412c9e2bccce0330d510] [generate_text] [9497668f447647a19326bf8cb99251cb] Started...\n",
      "19/Dec/2024:17:51:57 +0000 [INFO] [basics] [aeeffc3f63d04fb0940616b49f3a6548] [openai_lmm_invocation] [be45359b3a1b4e10b19fbed5825378ce] Started...\n",
      "19/Dec/2024:17:51:57 +0000 [INFO] [basics] [aeeffc3f63d04fb0940616b49f3a6548] [openai_lmm_invocation] [be45359b3a1b4e10b19fbed5825378ce] ...finished after 0.45s\n",
      "19/Dec/2024:17:51:57 +0000 [DEBUG] [basics] [b98eb4e0d921412c9e2bccce0330d510] [generate_text] [9497668f447647a19326bf8cb99251cb] Received text generation tool calls\n",
      "19/Dec/2024:17:51:57 +0000 [INFO] [basics] [cb13d113f1824e1bb0cf8abeb2506c91] [fun_fact] [b2f682d55a01461e97409308ebab9e67] Started...\n",
      "19/Dec/2024:17:51:57 +0000 [INFO] [basics] [cb13d113f1824e1bb0cf8abeb2506c91] [fun_fact] [b2f682d55a01461e97409308ebab9e67] ...finished after 0.00s\n",
      "19/Dec/2024:17:51:57 +0000 [INFO] [basics] [37ebd72b28e04dc0826f8a96d69c7d5f] [openai_lmm_invocation] [911f93f482f3432081a328f6ca168a77] Started...\n",
      "19/Dec/2024:17:51:59 +0000 [INFO] [basics] [37ebd72b28e04dc0826f8a96d69c7d5f] [openai_lmm_invocation] [911f93f482f3432081a328f6ca168a77] ...finished after 2.00s\n",
      "19/Dec/2024:17:51:59 +0000 [DEBUG] [basics] [b98eb4e0d921412c9e2bccce0330d510] [generate_text] [9497668f447647a19326bf8cb99251cb] Received text generation result\n",
      "19/Dec/2024:17:51:59 +0000 [INFO] [basics] [b98eb4e0d921412c9e2bccce0330d510] [generate_text] [9497668f447647a19326bf8cb99251cb] ...finished after 2.46s\n",
      "\n",
      "Result:\n",
      "The funniest thing about LLMs (Large Language Models) is that they can generate jokes and puns, but they have as much sense of humor as a rock at a comedy club! I mean, they might tell a joke so funny that even they don’t get it. Talk about a punchline that falls flat! Just remember, they don’t really \"get\" humor – they're just really good at mimicking it. So if you ever need a laugh, just remember: even machines can't recommend good comedy!\n",
      "\n",
      "19/Dec/2024:17:51:59 +0000 [INFO] [basics] [185e42d863da45869acc9702d6220949] [basics] [2e773dc58ff14e37a6398552b5766a96] ...finished after 2.46s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/Dec/2024:17:51:59 +0000 [INFO] [basics] [185e42d863da45869acc9702d6220949] [basics] [2e773dc58ff14e37a6398552b5766a96] Usage metrics:\n",
      "@basics[2e773dc58ff14e37a6398552b5766a96](2.46s):\n",
      "• TokenUsage:\n",
      "|  + usage: \n",
      "|  |  + gpt-4o-mini: \n",
      "|  |  |  + input_tokens: 151\n",
      "|  |  |  + output_tokens: 111\n",
      "@generate_text[9497668f447647a19326bf8cb99251cb](2.46s):\n",
      "|  • TokenUsage:\n",
      "|  |  + usage: \n",
      "|  |  |  + gpt-4o-mini: \n",
      "|  |  |  |  + input_tokens: 151\n",
      "|  |  |  |  + output_tokens: 111\n",
      "|  @openai_lmm_invocation[be45359b3a1b4e10b19fbed5825378ce](0.45s):\n",
      "|  |  • ArgumentsTrace:\n",
      "|  |  |  + kwargs: \n",
      "|  |  |  |  + instruction: You are a funny assistant\n",
      "|  |  |  |  + context: \n",
      "|  |  |  |  |  [0] content: \n",
      "|  |  |  |  |    parts: \n",
      "|  |  |  |  |      - text: What is the funny thing about LLMs?\n",
      "|  |  |  |  |        meta: None\n",
      "|  |  |  |  |  [1] requests: \n",
      "|  |  |  |  |    - identifier: call_H1TxwXL2YZvuOduib1Oh8H6r\n",
      "|  |  |  |  |      tool: fun_fact\n",
      "|  |  |  |  |      arguments: \n",
      "|  |  |  |  |        topic: language models\n",
      "|  |  |  |  |  [2] responses: \n",
      "|  |  |  |  |    - identifier: call_H1TxwXL2YZvuOduib1Oh8H6r\n",
      "|  |  |  |  |      tool: fun_fact\n",
      "|  |  |  |  |      content: \n",
      "|  |  |  |  |        parts: \n",
      "|  |  |  |  |          - text: language models is very funny on its own!\n",
      "|  |  |  |  |            meta: None\n",
      "|  |  |  |  |      direct: False\n",
      "|  |  |  |  |      error: False\n",
      "|  |  |  |  + tool_selection: \n",
      "|  |  |  |  |  + name: fun_fact\n",
      "|  |  |  |  |  + description: Find a fun fact in a given topic\n",
      "|  |  |  |  |  + parameters: \n",
      "|  |  |  |  |  |  + type: object\n",
      "|  |  |  |  |  |  + properties: \n",
      "|  |  |  |  |  |  |  + topic: \n",
      "|  |  |  |  |  |  |  |  + type: string\n",
      "|  |  |  |  |  |  |  |  + description: Topic of a fact to find\n",
      "|  |  |  |  + tools: \n",
      "|  |  |  |  |  [0] \n",
      "|  |  |  |  |  |  + name: current_time\n",
      "|  |  |  |  |  |  + description: None\n",
      "|  |  |  |  |  |  + parameters: \n",
      "|  |  |  |  |  |  |  + type: object\n",
      "|  |  |  |  |  |  |  + properties: \n",
      "|  |  |  |  |  |  |  |  + location: \n",
      "|  |  |  |  |  |  |  |  |  + type: string\n",
      "|  |  |  |  |  |  |  + required: \n",
      "|  |  |  |  |  |  |  |  [0] location\n",
      "|  |  |  |  |  [1] \n",
      "|  |  |  |  |  |  + name: fun_fact\n",
      "|  |  |  |  |  |  + description: Find a fun fact in a given topic\n",
      "|  |  |  |  |  |  + parameters: \n",
      "|  |  |  |  |  |  |  + type: object\n",
      "|  |  |  |  |  |  |  + properties: \n",
      "|  |  |  |  |  |  |  |  + topic: \n",
      "|  |  |  |  |  |  |  |  |  + type: string\n",
      "|  |  |  |  |  |  |  |  |  + description: Topic of a fact to find\n",
      "|  |  |  |  + output: text\n",
      "|  |  • OpenAIChatConfig:\n",
      "|  |  |  + model: gpt-4o-mini\n",
      "|  |  |  + temperature: 1.0\n",
      "|  |  • TokenUsage:\n",
      "|  |  |  + usage: \n",
      "|  |  |  |  + gpt-4o-mini: \n",
      "|  |  |  |  |  + input_tokens: 93\n",
      "|  |  |  |  |  + output_tokens: 6\n",
      "|  |  • OpenAISystemFingerprint:\n",
      "|  |  |  + system_fingerprint: fp_0aa8d3e20b\n",
      "|  |  • ResultTrace:\n",
      "|  |  |  + result: \n",
      "|  |  |  |  [0] ChatCompletionMessageToolCall(id='call_H1TxwXL2YZvuOduib1Oh8H6r', function=Function(arguments='{\"topic\":\"language models\"}', name='fun_fact'), type='function')\n",
      "|  @fun_fact[b2f682d55a01461e97409308ebab9e67](0.00s):\n",
      "|  |  • ArgumentsTrace:\n",
      "|  |  |  + kwargs: \n",
      "|  |  |  |  + topic: language models\n",
      "|  |  • ResultTrace:\n",
      "|  |  |  + result: language models is very funny on its own!\n",
      "|  @openai_lmm_invocation[911f93f482f3432081a328f6ca168a77](2.00s):\n",
      "|  |  • ArgumentsTrace:\n",
      "|  |  |  + kwargs: \n",
      "|  |  |  |  + instruction: You are a funny assistant\n",
      "|  |  |  |  + context: \n",
      "|  |  |  |  |  [0] content: \n",
      "|  |  |  |  |    parts: \n",
      "|  |  |  |  |      - text: What is the funny thing about LLMs?\n",
      "|  |  |  |  |        meta: None\n",
      "|  |  |  |  |  [1] requests: \n",
      "|  |  |  |  |    - identifier: call_H1TxwXL2YZvuOduib1Oh8H6r\n",
      "|  |  |  |  |      tool: fun_fact\n",
      "|  |  |  |  |      arguments: \n",
      "|  |  |  |  |        topic: language models\n",
      "|  |  |  |  |  [2] responses: \n",
      "|  |  |  |  |    - identifier: call_H1TxwXL2YZvuOduib1Oh8H6r\n",
      "|  |  |  |  |      tool: fun_fact\n",
      "|  |  |  |  |      content: \n",
      "|  |  |  |  |        parts: \n",
      "|  |  |  |  |          - text: language models is very funny on its own!\n",
      "|  |  |  |  |            meta: None\n",
      "|  |  |  |  |      direct: False\n",
      "|  |  |  |  |      error: False\n",
      "|  |  |  |  + tool_selection: none\n",
      "|  |  |  |  + tools: \n",
      "|  |  |  |  |  [0] \n",
      "|  |  |  |  |  |  + name: current_time\n",
      "|  |  |  |  |  |  + description: None\n",
      "|  |  |  |  |  |  + parameters: \n",
      "|  |  |  |  |  |  |  + type: object\n",
      "|  |  |  |  |  |  |  + properties: \n",
      "|  |  |  |  |  |  |  |  + location: \n",
      "|  |  |  |  |  |  |  |  |  + type: string\n",
      "|  |  |  |  |  |  |  + required: \n",
      "|  |  |  |  |  |  |  |  [0] location\n",
      "|  |  |  |  |  [1] \n",
      "|  |  |  |  |  |  + name: fun_fact\n",
      "|  |  |  |  |  |  + description: Find a fun fact in a given topic\n",
      "|  |  |  |  |  |  + parameters: \n",
      "|  |  |  |  |  |  |  + type: object\n",
      "|  |  |  |  |  |  |  + properties: \n",
      "|  |  |  |  |  |  |  |  + topic: \n",
      "|  |  |  |  |  |  |  |  |  + type: string\n",
      "|  |  |  |  |  |  |  |  |  + description: Topic of a fact to find\n",
      "|  |  |  |  + output: text\n",
      "|  |  • OpenAIChatConfig:\n",
      "|  |  |  + model: gpt-4o-mini\n",
      "|  |  |  + temperature: 1.0\n",
      "|  |  • TokenUsage:\n",
      "|  |  |  + usage: \n",
      "|  |  |  |  + gpt-4o-mini: \n",
      "|  |  |  |  |  + input_tokens: 58\n",
      "|  |  |  |  |  + output_tokens: 105\n",
      "|  |  • OpenAISystemFingerprint:\n",
      "|  |  |  + system_fingerprint: fp_39a40c96a0\n",
      "|  |  • ResultTrace:\n",
      "|  |  |  + result: The funniest thing about LLMs (Large Language Models) is that they can generate jokes and puns, but they have as much sense of humor as a rock at a comedy club! I mean, they might tell a joke so funny that even they don’t get it. Talk about a punchline that falls flat! Just remember, they don’t really \"get\" humor – they're just really good at mimicking it. So if you ever need a laugh, just remember: even machines can't recommend good comedy!\n"
     ]
    }
   ],
   "source": [
    "from draive import setup_logging, usage_metrics_logger\n",
    "\n",
    "setup_logging(\"basics\")\n",
    "\n",
    "async with ctx.scope(\n",
    "    \"basics\",\n",
    "    # define used LMM to be OpenAI within the context\n",
    "    openai_lmm(),\n",
    "    OpenAIChatConfig(model=\"gpt-4o-mini\"),\n",
    "    completion=usage_metrics_logger(),\n",
    "):\n",
    "    result: str = await generate_text(\n",
    "        instruction=\"You are a funny assistant\",\n",
    "        input=\"What is the funny thing about LLMs?\",\n",
    "        # we will now be able to see what tools were used\n",
    "        # and check the details about its execution\n",
    "        tools=Toolbox.of(\n",
    "            current_time,\n",
    "            suggest=customized,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(f\"\\nResult:\\n{result}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
