{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tools use\n",
    "\n",
    "Various LLM use cases can utilize function/tool calls to extend LLM capabilities. Draive comes with a dedicated solution to prepare tools and control its execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool definition\n",
    "\n",
    "Let's start by defining a simple tool. Tool is usually a function which is explained to LLM and can be requested to be used. Tools can require generation of arguments and have to return some value. Tools defined within draive are python async function annotated with `tool` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draive import tool\n",
    "\n",
    "\n",
    "@tool # simply annotate function as a tool, tools can have arguments using basic types\n",
    "async def current_time(location: str) -> str:\n",
    "    # return result of tool execution, we are using fake time here\n",
    "    return f\"Time in {location} is 9:53:22\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool use\n",
    "\n",
    "After defining a tool we can use it in multiple scenarios to extend LLM capabilities. All of the tool arguments will be validated when calling. It includes type check and any additional validation if defined. You can still use it as a regular function, although, all tools have to be executed within draive context scope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time in London is 9:53:22\n"
     ]
    }
   ],
   "source": [
    "from draive import ctx\n",
    "\n",
    "async with ctx.scope(\"basics\"):\n",
    "    # we can still use it as a regular function\n",
    "    # but it has to be executed within context scope\n",
    "    print(await current_time(location=\"London\"))\n",
    "\n",
    "# await current_time(location=\"London\") # error! out of context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest benefit of defining a tool is when using LLM. We can tell the model to use available tools to extend its capabilities. Higher level interfaces automatically handle tool calls and going back with its result to LLM to receive the final result. We can see how it works within a simple text generator. Tools are provided within an iterable collection like list or tuple or within the `Toolbox` object which allows customizing tools execution. You can prepare tools collection each time it is used or reuse preconstructed one. We will use OpenAI GPT model as it natively supports tool use. Make sure to provide .env file with `OPENAI_API_KEY` key before running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current time in New York is 9:53 AM.\n"
     ]
    }
   ],
   "source": [
    "from draive import generate_text, load_env\n",
    "from draive.openai import OpenAIChatConfig, openai_lmm\n",
    "\n",
    "load_env()\n",
    "\n",
    "async with ctx.scope(\n",
    "    \"basics\",\n",
    "    # define used LMM to be OpenAI within the context\n",
    "    openai_lmm(),\n",
    "    OpenAIChatConfig(model=\"gpt-4o-mini\"),\n",
    "):\n",
    "    result: str = await generate_text(\n",
    "        instruction=\"You are a helpful assistant\",\n",
    "        input=\"What is the time in New York?\",\n",
    "        tools=[current_time], # or `Toolbox(current_time)`\n",
    "    )\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool details\n",
    "\n",
    "Tools can be customized and extended in various ways depending on use case. First of all we can customize tool arguments and help LLM to better understand how to use given tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'fun_fact', 'description': 'Find a fun fact in a given topic', 'parameters': {'type': 'object', 'properties': {'topic': {'type': 'string', 'description': 'Topic of a fact to find'}}, 'required': []}}\n"
     ]
    }
   ],
   "source": [
    "from draive import Argument\n",
    "\n",
    "\n",
    "@tool( # this time we will use additional arguments within tool annotation\n",
    "    # we can define an alias used as the tool name when explaining it to LLM,\n",
    "    # default name is the name of python function\n",
    "    name=\"fun_fact\",\n",
    "    # additionally we can explain the tool purpose by using description\n",
    "    description=\"Find a fun fact in a given topic\",\n",
    ")\n",
    "async def customized(\n",
    "    # we can also annotate arguments to provide even more details\n",
    "    # and specify argument handling logic\n",
    "    arg: str = Argument(\n",
    "        # we can alias each argument name\n",
    "        aliased=\"topic\",\n",
    "        # further describe its use\n",
    "        description=\"Topic of a fact to find\",\n",
    "        # provide default value or default value factory\n",
    "        default=\"random\",\n",
    "        # end more, including custom validators\n",
    "    ),\n",
    ") -> str:\n",
    "    return f\"{arg} is very funny on its own!\"\n",
    "\n",
    "# we can examine tool specification which is similar to\n",
    "# how `State` and `DataModel` specification/schema is built\n",
    "print(customized.specification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can customize tools even more using additional parameters like custom result formatting or requiring direct result from tool when used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool( # this time we will use different arguments within tool annotation\n",
    "    # direct result allows returning the result without additional LLM call\n",
    "    direct_result=True,\n",
    "    # format_result allows altering the way LLMs will see tool results\n",
    "    # we can also use format_failure to format tool issues\n",
    "    format_result=lambda result: f\"Formatted: {result}\",\n",
    "    # we can also contextually limit tool availability in runtime\n",
    "    # by removing it from available tools list on given condition\n",
    "    availability_check=lambda: True\n",
    ")\n",
    "async def customized_more() -> str:\n",
    "    return \"to be changed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toolbox\n",
    "\n",
    "We have already mentioned the Toolbox which allows us to specify some additional details regarding the tools execution like the tool calls limit or a tool suggestion. Here is how it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The funny thing about language models is that they can generate text that sometimes makes perfect sense, but other times they create sentences that are completely nonsensical! It's like having a conversation with a really smart parrot that also had one too many cups of coffee. You never know when itâ€™s going to spill out something profound or just squawk like a confused bird! ðŸ¦œâœ¨\n"
     ]
    }
   ],
   "source": [
    "from draive import Toolbox\n",
    "\n",
    "async with ctx.scope(\n",
    "    \"basics\",\n",
    "    # define used LMM to be OpenAI within the context\n",
    "    openai_lmm(),\n",
    "    OpenAIChatConfig(model=\"gpt-4o-mini\"),\n",
    "):\n",
    "    result: str = await generate_text(\n",
    "        instruction=\"You are a funny assistant\",\n",
    "        input=\"What is the funny thing about LLMs?\",\n",
    "        tools=Toolbox.of(\n",
    "            # we can define any number of tools within a toolbox\n",
    "            current_time,\n",
    "            # we can also force any given tool use within the first LLM call\n",
    "            suggest=customized,\n",
    "            # we can limit how many tool calls are allowed\n",
    "            # before the final result is returned\n",
    "            repeated_calls_limit=2,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "All of the tool usage is automatically traced within scope metrics. We can see the details about their execution when using a logger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/Jan/2025:15:33:53 +0000 [DEBUG] [basics] [71a63169a02b45e6a8c5221a266fb1ee] [basics] [c6160541e7364c1385fa60d8b980c174] Entering context...\n",
      "\n",
      "Result:\n",
      "Well, the funny thing about LLMs (Large Language Models) is that they can generate text that sounds incredibly intelligentâ€”right up until they suggest that the best way to fix your computer is to give it a nice warm cup of tea! Itâ€™s like having a super-smart friend whoâ€™s also a little clueless about real life. You'll never know when theyâ€™ll pop out with the most profound wisdomâ€¦ or the most questionable advice about cat yoga!\n",
      "\n",
      "07/Jan/2025:15:33:56 +0000 [DEBUG] [basics] [71a63169a02b45e6a8c5221a266fb1ee] [basics] [c6160541e7364c1385fa60d8b980c174] Metrics summary:\n",
      "âŽ¡ @basics [c6160541e7364c1385fa60d8b980c174](2.86s):\n",
      "|  \n",
      "|  âŽ¡ @generate_text [1a375d0c8a7242a0b943e9ddb4f4c1a9](2.86s):\n",
      "|  |  \n",
      "|  |  âŽ¡ @openai_lmm_invocation [21da2672ab9a4cd8b3885012741afac8](0.61s):\n",
      "|  |  |  âŽ¡ â€¢ArgumentsTrace:\n",
      "|  |  |  |  â”œ kwargs: \n",
      "|  |  |  |  |  [instruction]: \"You are a funny assistant\"\n",
      "|  |  |  |  |  [context]: \n",
      "|  |  |  |  |  |  [0] content: \n",
      "|  |  |  |  |  |  |    parts: \n",
      "|  |  |  |  |  |  |      - text: What is the funny thing about LLMs?\n",
      "|  |  |  |  |  |  |        meta: None\n",
      "|  |  |  |  |  |  [1] requests: \n",
      "|  |  |  |  |  |  |    - identifier: call_rCGt030OSf0FgNjVMiFrUwfI\n",
      "|  |  |  |  |  |  |      tool: fun_fact\n",
      "|  |  |  |  |  |  |      arguments: \n",
      "|  |  |  |  |  |  |        topic: LLMs\n",
      "|  |  |  |  |  |  [2] responses: \n",
      "|  |  |  |  |  |  |    - identifier: call_rCGt030OSf0FgNjVMiFrUwfI\n",
      "|  |  |  |  |  |  |      tool: fun_fact\n",
      "|  |  |  |  |  |  |      content: \n",
      "|  |  |  |  |  |  |        parts: \n",
      "|  |  |  |  |  |  |          - text: LLMs is very funny on its own!\n",
      "|  |  |  |  |  |  |            meta: None\n",
      "|  |  |  |  |  |  |      direct: False\n",
      "|  |  |  |  |  |  |      error: False\n",
      "|  |  |  |  |  [tool_selection]: \n",
      "|  |  |  |  |  |  [name]: \"fun_fact\"\n",
      "|  |  |  |  |  |  [description]: \"Find a fun fact in a given topic\"\n",
      "|  |  |  |  |  |  [parameters]: \n",
      "|  |  |  |  |  |  |  [type]: \"object\"\n",
      "|  |  |  |  |  |  |  [properties]: \n",
      "|  |  |  |  |  |  |  |  [topic]: \n",
      "|  |  |  |  |  |  |  |  |  [type]: \"string\"\n",
      "|  |  |  |  |  |  |  |  |  [description]: \"Topic of a fact to find\"\n",
      "|  |  |  |  |  [tools]: \n",
      "|  |  |  |  |  |  [0] \n",
      "|  |  |  |  |  |  |  [name]: \"current_time\"\n",
      "|  |  |  |  |  |  |  [description]: None\n",
      "|  |  |  |  |  |  |  [parameters]: \n",
      "|  |  |  |  |  |  |  |  [type]: \"object\"\n",
      "|  |  |  |  |  |  |  |  [properties]: \n",
      "|  |  |  |  |  |  |  |  |  [location]: \n",
      "|  |  |  |  |  |  |  |  |  |  [type]: \"string\"\n",
      "|  |  |  |  |  |  |  |  [required]: \n",
      "|  |  |  |  |  |  |  |  |  [0] \"location\"\n",
      "|  |  |  |  |  |  [1] \n",
      "|  |  |  |  |  |  |  [name]: \"fun_fact\"\n",
      "|  |  |  |  |  |  |  [description]: \"Find a fun fact in a given topic\"\n",
      "|  |  |  |  |  |  |  [parameters]: \n",
      "|  |  |  |  |  |  |  |  [type]: \"object\"\n",
      "|  |  |  |  |  |  |  |  [properties]: \n",
      "|  |  |  |  |  |  |  |  |  [topic]: \n",
      "|  |  |  |  |  |  |  |  |  |  [type]: \"string\"\n",
      "|  |  |  |  |  |  |  |  |  |  [description]: \"Topic of a fact to find\"\n",
      "|  |  |  |  |  [output]: \"text\"\n",
      "|  |  |  âŒŠ\n",
      "|  |  |  âŽ¡ â€¢OpenAIChatConfig:\n",
      "|  |  |  |  â”œ model: \"gpt-4o-mini\"\n",
      "|  |  |  |  â”œ temperature: 1.0\n",
      "|  |  |  âŒŠ\n",
      "|  |  |  âŽ¡ â€¢TokenUsage:\n",
      "|  |  |  |  â”œ usage: \n",
      "|  |  |  |  |  [gpt-4o-mini]: \n",
      "|  |  |  |  |  â”œ input_tokens: 93\n",
      "|  |  |  |  |  â”œ output_tokens: 7\n",
      "|  |  |  âŒŠ\n",
      "|  |  |  âŽ¡ â€¢OpenAISystemFingerprint:\n",
      "|  |  |  |  â”œ system_fingerprint: \"fp_0aa8d3e20b\"\n",
      "|  |  |  âŒŠ\n",
      "|  |  |  âŽ¡ â€¢ResultTrace:\n",
      "|  |  |  |  â”œ result: \n",
      "|  |  |  |  |  [0] ChatCompletionMessageToolCall(id='call_rCGt030OSf0FgNjVMiFrUwfI', function=Function(arguments='{\"topic\":\"LLMs\"}', name='fun_fact'), type='function')\n",
      "|  |  |  âŒŠ\n",
      "|  |  âŒŠ\n",
      "|  |  \n",
      "|  |  âŽ¡ @fun_fact [1b09fe452e374f379a1edcf06390237a](0.00s):\n",
      "|  |  |  âŽ¡ â€¢ArgumentsTrace:\n",
      "|  |  |  |  â”œ kwargs: \n",
      "|  |  |  |  |  [topic]: \"LLMs\"\n",
      "|  |  |  âŒŠ\n",
      "|  |  |  âŽ¡ â€¢ResultTrace:\n",
      "|  |  |  |  â”œ result: \"LLMs is very funny on its own!\"\n",
      "|  |  |  âŒŠ\n",
      "|  |  âŒŠ\n",
      "|  |  \n",
      "|  |  âŽ¡ @openai_lmm_invocation [256f5cd2b36448509732a36fce87508b](2.25s):\n",
      "|  |  |  âŽ¡ â€¢ArgumentsTrace:\n",
      "|  |  |  |  â”œ kwargs: \n",
      "|  |  |  |  |  [instruction]: \"You are a funny assistant\"\n",
      "|  |  |  |  |  [context]: \n",
      "|  |  |  |  |  |  [0] content: \n",
      "|  |  |  |  |  |  |    parts: \n",
      "|  |  |  |  |  |  |      - text: What is the funny thing about LLMs?\n",
      "|  |  |  |  |  |  |        meta: None\n",
      "|  |  |  |  |  |  [1] requests: \n",
      "|  |  |  |  |  |  |    - identifier: call_rCGt030OSf0FgNjVMiFrUwfI\n",
      "|  |  |  |  |  |  |      tool: fun_fact\n",
      "|  |  |  |  |  |  |      arguments: \n",
      "|  |  |  |  |  |  |        topic: LLMs\n",
      "|  |  |  |  |  |  [2] responses: \n",
      "|  |  |  |  |  |  |    - identifier: call_rCGt030OSf0FgNjVMiFrUwfI\n",
      "|  |  |  |  |  |  |      tool: fun_fact\n",
      "|  |  |  |  |  |  |      content: \n",
      "|  |  |  |  |  |  |        parts: \n",
      "|  |  |  |  |  |  |          - text: LLMs is very funny on its own!\n",
      "|  |  |  |  |  |  |            meta: None\n",
      "|  |  |  |  |  |  |      direct: False\n",
      "|  |  |  |  |  |  |      error: False\n",
      "|  |  |  |  |  [tool_selection]: \"none\"\n",
      "|  |  |  |  |  [tools]: \n",
      "|  |  |  |  |  |  [0] \n",
      "|  |  |  |  |  |  |  [name]: \"current_time\"\n",
      "|  |  |  |  |  |  |  [description]: None\n",
      "|  |  |  |  |  |  |  [parameters]: \n",
      "|  |  |  |  |  |  |  |  [type]: \"object\"\n",
      "|  |  |  |  |  |  |  |  [properties]: \n",
      "|  |  |  |  |  |  |  |  |  [location]: \n",
      "|  |  |  |  |  |  |  |  |  |  [type]: \"string\"\n",
      "|  |  |  |  |  |  |  |  [required]: \n",
      "|  |  |  |  |  |  |  |  |  [0] \"location\"\n",
      "|  |  |  |  |  |  [1] \n",
      "|  |  |  |  |  |  |  [name]: \"fun_fact\"\n",
      "|  |  |  |  |  |  |  [description]: \"Find a fun fact in a given topic\"\n",
      "|  |  |  |  |  |  |  [parameters]: \n",
      "|  |  |  |  |  |  |  |  [type]: \"object\"\n",
      "|  |  |  |  |  |  |  |  [properties]: \n",
      "|  |  |  |  |  |  |  |  |  [topic]: \n",
      "|  |  |  |  |  |  |  |  |  |  [type]: \"string\"\n",
      "|  |  |  |  |  |  |  |  |  |  [description]: \"Topic of a fact to find\"\n",
      "|  |  |  |  |  [output]: \"text\"\n",
      "|  |  |  âŒŠ\n",
      "|  |  |  âŽ¡ â€¢OpenAIChatConfig:\n",
      "|  |  |  |  â”œ model: \"gpt-4o-mini\"\n",
      "|  |  |  |  â”œ temperature: 1.0\n",
      "|  |  |  âŒŠ\n",
      "|  |  |  âŽ¡ â€¢TokenUsage:\n",
      "|  |  |  |  â”œ usage: \n",
      "|  |  |  |  |  [gpt-4o-mini]: \n",
      "|  |  |  |  |  â”œ input_tokens: 58\n",
      "|  |  |  |  |  â”œ output_tokens: 91\n",
      "|  |  |  âŒŠ\n",
      "|  |  |  âŽ¡ â€¢OpenAISystemFingerprint:\n",
      "|  |  |  |  â”œ system_fingerprint: \"fp_0aa8d3e20b\"\n",
      "|  |  |  âŒŠ\n",
      "|  |  |  âŽ¡ â€¢ResultTrace:\n",
      "|  |  |  |  â”œ result: \"Well, the funny thing about LLMs (Large Language Models) is that they can generate text that sounds incredibly intelligentâ€”right up until they suggest that the best way to fix your computer is to give it a nice warm cup of tea! Itâ€™s like having a super-smart friend whoâ€™s also a little clueless about real life. You'll never know when theyâ€™ll pop out with the most profound wisdomâ€¦ or the most questionable advice about cat yoga!\"\n",
      "|  |  |  âŒŠ\n",
      "|  |  âŒŠ\n",
      "|  âŒŠ\n",
      "âŒŠ\n",
      "07/Jan/2025:15:33:56 +0000 [DEBUG] [basics] [71a63169a02b45e6a8c5221a266fb1ee] [basics] [c6160541e7364c1385fa60d8b980c174] ...exiting context after 2.86s\n"
     ]
    }
   ],
   "source": [
    "from draive import MetricsLogger, setup_logging\n",
    "\n",
    "setup_logging(\"basics\")\n",
    "\n",
    "async with ctx.scope(\n",
    "    \"basics\",\n",
    "    # define used LMM to be OpenAI within the context\n",
    "    openai_lmm(),\n",
    "    OpenAIChatConfig(model=\"gpt-4o-mini\"),\n",
    "    metrics=MetricsLogger.handler(),\n",
    "):\n",
    "    result: str = await generate_text(\n",
    "        instruction=\"You are a funny assistant\",\n",
    "        input=\"What is the funny thing about LLMs?\",\n",
    "        # we will now be able to see what tools were used\n",
    "        # and check the details about its execution\n",
    "        tools=Toolbox.of(\n",
    "            current_time,\n",
    "            suggest=customized,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(f\"\\nResult:\\n{result}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
