{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tools use\n",
    "\n",
    "Various LLM use cases can utilize function/tool calls to extend LLM capabilities. Draive comes with a dedicated solution to prepare tools and control its execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool definition\n",
    "\n",
    "Let's start by defining a simple tool. Tool is usually a function which is explained to LLM and can be requested to be used. Tools can require generation of arguments and have to return some value. Tools defined within draive are python async function annotated with `tool` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draive import tool\n",
    "\n",
    "\n",
    "@tool # simply annotate function as a tool, tools can have arguments using basic types\n",
    "async def current_time(location: str) -> str:\n",
    "    # return result of tool execution, we are using fake time here\n",
    "    return f\"Time in {location} is 9:53:22\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool use\n",
    "\n",
    "After defining a tool we can use it in multiple scenarios to extend LLM capabilities. All of the tool arguments will be validated when calling. It includes type check and any additional validation if defined. You can still use it as a regular function, although, all tools have to be executed within draive context scope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time in London is 9:53:22\n"
     ]
    }
   ],
   "source": [
    "from draive import ctx\n",
    "\n",
    "async with ctx.scope(\"basics\"):\n",
    "    # we can still use it as a regular function\n",
    "    # but it has to be executed within context scope\n",
    "    print(await current_time(location=\"London\"))\n",
    "\n",
    "# await current_time(location=\"London\") # error! out of context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest benefit of defining a tool is when using LLM. We can tell the model to use available tools to extend its capabilities. Higher level interfaces automatically handle tool calls and going back with its result to LLM to receive the final result. We can see how it works within a simple text generator. Tools are provided within a `Toolbox` object which allows customizing tools execution. You can prepare a `Toolbox` each time it is used or reuse preconstructed one. We will use OpenAI GPT model as it natively supports tool use. Make sure to provide .env file with `OPENAI_API_KEY` key before running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current time in New York is 9:53 AM.\n"
     ]
    }
   ],
   "source": [
    "from draive import LMM, Toolbox, generate_text, load_env\n",
    "from draive.openai import openai_lmm\n",
    "\n",
    "load_env()\n",
    "\n",
    "async with ctx.scope(\n",
    "    \"basics\",\n",
    "    # define used LMM to be OpenAI within the context\n",
    "    LMM(invocation=openai_lmm()),\n",
    "):\n",
    "    result: str = await generate_text(\n",
    "        instruction=\"You are a helpful assistant\",\n",
    "        input=\"What is the time in New York?\",\n",
    "        tools=Toolbox(current_time),\n",
    "    )\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool details\n",
    "\n",
    "Tools can be customized and extended in various ways depending on use case. First of all we can customize tool arguments and help LLM to better understand how to use given tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'function', 'function': {'name': 'fun_fact', 'parameters': {'type': 'object', 'properties': {'topic': {'type': 'string', 'description': 'Topic of a fact to find'}}, 'required': []}, 'description': 'Find a fun fact in a given topic'}}\n"
     ]
    }
   ],
   "source": [
    "from draive import Argument\n",
    "\n",
    "\n",
    "@tool( # this time we will use additional arguments within tool annotation\n",
    "    # we can define an alias used as the tool name when explaining it to LLM,\n",
    "    # default name is the name of python function\n",
    "    name=\"fun_fact\",\n",
    "    # additionally we can explain the tool purpose by using description\n",
    "    description=\"Find a fun fact in a given topic\",\n",
    ")\n",
    "async def customized(\n",
    "    # we can also annotate arguments to provide even more details\n",
    "    # and specify argument handling logic\n",
    "    arg: str = Argument(\n",
    "        # we can alias each argument name\n",
    "        aliased=\"topic\",\n",
    "        # further describe its use\n",
    "        description=\"Topic of a fact to find\",\n",
    "        # provide default value or default value factory\n",
    "        default=\"random\",\n",
    "        # end more, including custom validators\n",
    "    ),\n",
    ") -> str:\n",
    "    return f\"{arg} is very funny on its own!\"\n",
    "\n",
    "# we can examine tool specification which is similar to\n",
    "# how `State` and `DataModel` specification/schema is built\n",
    "print(customized.specification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toolbox\n",
    "\n",
    "We have already used a Toolbox to ask for an extended LLM completion. However Toolbox allows us to specify some additional details regarding the tools execution like the tool calls limit or a tool suggestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The funny thing about LLMs (Large Language Models) is that they can generate text that sounds remarkably intelligent, yet sometimes they mix up facts or produce hilariously unexpected responses! It's like having a very smart friend who's just a little bit quirky and occasionally forgets that cats can’t actually take your dog for a walk!\n"
     ]
    }
   ],
   "source": [
    "async with ctx.scope(\n",
    "    \"basics\",\n",
    "    # define used LMM to be OpenAI within the context\n",
    "    LMM(invocation=openai_lmm()),\n",
    "):\n",
    "    result: str = await generate_text(\n",
    "        instruction=\"You are a funny assistant\",\n",
    "        input=\"What is the funny thing about LLMs?\",\n",
    "        tools=Toolbox(\n",
    "            # we can define any number of tools within a toolbox\n",
    "            current_time,\n",
    "            customized,\n",
    "            # we can limit how many tool calls are allowed\n",
    "            # before the final result is returned\n",
    "            recursive_calls_limit=2,\n",
    "            # we can also force any given tool use within the first LLM call\n",
    "            suggest=customized,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "All of the tool usage is automatically traced within scope metrics. We can see the details about their execution when using a logger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/Oct/2024:10:37:15 +0000 [DEBUG] [basics] [ba5ffb0ba4e3446ca9db28fafe98b028] [generate_text] [a2c08297a7f840c78e858f054e3a4c46] Received text generation tool calls\n",
      "30/Oct/2024:10:37:17 +0000 [DEBUG] [basics] [ba5ffb0ba4e3446ca9db28fafe98b028] [generate_text] [a2c08297a7f840c78e858f054e3a4c46] Received text generation result\n",
      "\n",
      "Result:\n",
      "The funny thing about LLMs is that they can generate text that sounds super intelligent, but sometimes they mix up the facts like a distracted chef adding salt instead of sugar to a cake recipe. You might get a well-articulated essay on climate change, followed by a convincing argument that penguins are the real rulers of Antarctica! You never quite know when you're about to get Shakespeare or a penguin conspiracy theory!\n",
      "\n",
      "30/Oct/2024:10:37:17 +0000 [INFO] [basics] [ba5ffb0ba4e3446ca9db28fafe98b028] [basics] [187ffd0bee7d40e9831d044d86599248] Usage metrics:\n",
      "@basics[187ffd0bee7d40e9831d044d86599248](2.41s):\n",
      "• TokenUsage:\n",
      "|  + usage: \n",
      "|  |  + gpt-4o-mini: \n",
      "|  |  |  + input_tokens: 151\n",
      "|  |  |  + output_tokens: 90\n",
      "@generate_text[a2c08297a7f840c78e858f054e3a4c46](2.41s):\n",
      "|  • TokenUsage:\n",
      "|  |  + usage: \n",
      "|  |  |  + gpt-4o-mini: \n",
      "|  |  |  |  + input_tokens: 209\n",
      "|  |  |  |  + output_tokens: 174\n",
      "|  @openai_lmm_invocation[de331496a9f34020860773594cda2955](2.41s):\n",
      "|  |  • ArgumentsTrace:\n",
      "|  |  |  + kwargs: \n",
      "|  |  |  |  + instruction: You are a funny assistant\n",
      "|  |  |  |  + context: \n",
      "|  |  |  |  |  [0] content: \n",
      "|  |  |  |  |    parts: \n",
      "|  |  |  |  |      - text: What is the funny thing about LLMs?\n",
      "|  |  |  |  |        meta: None\n",
      "|  |  |  |  |  [1] requests: \n",
      "|  |  |  |  |    - identifier: call_wfTuMpYUEePFwERN8WJQWCEn\n",
      "|  |  |  |  |      tool: fun_fact\n",
      "|  |  |  |  |      arguments: \n",
      "|  |  |  |  |        topic: LLMs\n",
      "|  |  |  |  |  [2] identifier: call_wfTuMpYUEePFwERN8WJQWCEn\n",
      "|  |  |  |  |  tool: fun_fact\n",
      "|  |  |  |  |  content: \n",
      "|  |  |  |  |    parts: \n",
      "|  |  |  |  |      - text: LLMs is very funny on its own!\n",
      "|  |  |  |  |        meta: None\n",
      "|  |  |  |  |  direct: False\n",
      "|  |  |  |  |  error: False\n",
      "|  |  |  |  + tool_selection: \n",
      "|  |  |  |  |  + type: function\n",
      "|  |  |  |  |  + function: \n",
      "|  |  |  |  |  |  + name: fun_fact\n",
      "|  |  |  |  |  |  + parameters: \n",
      "|  |  |  |  |  |  |  + type: object\n",
      "|  |  |  |  |  |  |  + properties: \n",
      "|  |  |  |  |  |  |  |  + topic: \n",
      "|  |  |  |  |  |  |  |  |  + type: string\n",
      "|  |  |  |  |  |  |  |  |  + description: Topic of a fact to find\n",
      "|  |  |  |  |  |  + description: Find a fun fact in a given topic\n",
      "|  |  |  |  + tools: \n",
      "|  |  |  |  |  [0] \n",
      "|  |  |  |  |  |  + type: function\n",
      "|  |  |  |  |  |  + function: \n",
      "|  |  |  |  |  |  |  + name: current_time\n",
      "|  |  |  |  |  |  |  + parameters: \n",
      "|  |  |  |  |  |  |  |  + type: object\n",
      "|  |  |  |  |  |  |  |  + properties: \n",
      "|  |  |  |  |  |  |  |  |  + location: \n",
      "|  |  |  |  |  |  |  |  |  |  + type: string\n",
      "|  |  |  |  |  |  |  |  + required: \n",
      "|  |  |  |  |  |  |  |  |  [0] location\n",
      "|  |  |  |  |  [1] \n",
      "|  |  |  |  |  |  + type: function\n",
      "|  |  |  |  |  |  + function: \n",
      "|  |  |  |  |  |  |  + name: fun_fact\n",
      "|  |  |  |  |  |  |  + parameters: \n",
      "|  |  |  |  |  |  |  |  + type: object\n",
      "|  |  |  |  |  |  |  |  + properties: \n",
      "|  |  |  |  |  |  |  |  |  + topic: \n",
      "|  |  |  |  |  |  |  |  |  |  + type: string\n",
      "|  |  |  |  |  |  |  |  |  |  + description: Topic of a fact to find\n",
      "|  |  |  |  |  |  |  + description: Find a fun fact in a given topic\n",
      "|  |  |  |  + output: text\n",
      "|  |  • OpenAIChatConfig:\n",
      "|  |  |  + model: gpt-4o-mini\n",
      "|  |  |  + temperature: 1.0\n",
      "|  |  • TokenUsage:\n",
      "|  |  |  + usage: \n",
      "|  |  |  |  + gpt-4o-mini: \n",
      "|  |  |  |  |  + input_tokens: 209\n",
      "|  |  |  |  |  + output_tokens: 174\n",
      "|  |  • OpenAISystemFingerprint:\n",
      "|  |  |  + system_fingerprint: fp_f59a81427f\n",
      "|  |  • ResultTrace:\n",
      "|  |  |  + result: \n",
      "|  |  |  |  [0] ChatCompletionMessageToolCall(id='call_wfTuMpYUEePFwERN8WJQWCEn', function=Function(arguments='{\"topic\":\"LLMs\"}', name='fun_fact'), type='function')\n",
      "|  @fun_fact[038aa1d257e043c6ae4c1dd2262c9e1d](1.80s):\n",
      "|  |  • ArgumentsTrace:\n",
      "|  |  |  + kwargs: \n",
      "|  |  |  |  + topic: LLMs\n",
      "|  |  • ResultTrace:\n",
      "|  |  |  + result: LLMs is very funny on its own!\n",
      "|  @openai_lmm_invocation[2ba37ef86fe940bbb6864eba2bbc5271](1.80s):\n",
      "|  |  • ArgumentsTrace:\n",
      "|  |  |  + kwargs: \n",
      "|  |  |  |  + instruction: You are a funny assistant\n",
      "|  |  |  |  + context: \n",
      "|  |  |  |  |  [0] content: \n",
      "|  |  |  |  |    parts: \n",
      "|  |  |  |  |      - text: What is the funny thing about LLMs?\n",
      "|  |  |  |  |        meta: None\n",
      "|  |  |  |  |  [1] requests: \n",
      "|  |  |  |  |    - identifier: call_wfTuMpYUEePFwERN8WJQWCEn\n",
      "|  |  |  |  |      tool: fun_fact\n",
      "|  |  |  |  |      arguments: \n",
      "|  |  |  |  |        topic: LLMs\n",
      "|  |  |  |  |  [2] identifier: call_wfTuMpYUEePFwERN8WJQWCEn\n",
      "|  |  |  |  |  tool: fun_fact\n",
      "|  |  |  |  |  content: \n",
      "|  |  |  |  |    parts: \n",
      "|  |  |  |  |      - text: LLMs is very funny on its own!\n",
      "|  |  |  |  |        meta: None\n",
      "|  |  |  |  |  direct: False\n",
      "|  |  |  |  |  error: False\n",
      "|  |  |  |  + tool_selection: none\n",
      "|  |  |  |  + tools: \n",
      "|  |  |  |  |  [0] \n",
      "|  |  |  |  |  |  + type: function\n",
      "|  |  |  |  |  |  + function: \n",
      "|  |  |  |  |  |  |  + name: current_time\n",
      "|  |  |  |  |  |  |  + parameters: \n",
      "|  |  |  |  |  |  |  |  + type: object\n",
      "|  |  |  |  |  |  |  |  + properties: \n",
      "|  |  |  |  |  |  |  |  |  + location: \n",
      "|  |  |  |  |  |  |  |  |  |  + type: string\n",
      "|  |  |  |  |  |  |  |  + required: \n",
      "|  |  |  |  |  |  |  |  |  [0] location\n",
      "|  |  |  |  |  [1] \n",
      "|  |  |  |  |  |  + type: function\n",
      "|  |  |  |  |  |  + function: \n",
      "|  |  |  |  |  |  |  + name: fun_fact\n",
      "|  |  |  |  |  |  |  + parameters: \n",
      "|  |  |  |  |  |  |  |  + type: object\n",
      "|  |  |  |  |  |  |  |  + properties: \n",
      "|  |  |  |  |  |  |  |  |  + topic: \n",
      "|  |  |  |  |  |  |  |  |  |  + type: string\n",
      "|  |  |  |  |  |  |  |  |  |  + description: Topic of a fact to find\n",
      "|  |  |  |  |  |  |  + description: Find a fun fact in a given topic\n",
      "|  |  |  |  + output: text\n",
      "|  |  • OpenAIChatConfig:\n",
      "|  |  |  + model: gpt-4o-mini\n",
      "|  |  |  + temperature: 1.0\n",
      "|  |  • TokenUsage:\n",
      "|  |  |  + usage: \n",
      "|  |  |  |  + gpt-4o-mini: \n",
      "|  |  |  |  |  + input_tokens: 58\n",
      "|  |  |  |  |  + output_tokens: 84\n",
      "|  |  • OpenAISystemFingerprint:\n",
      "|  |  |  + system_fingerprint: fp_f59a81427f\n",
      "|  |  • ResultTrace:\n",
      "|  |  |  + result: The funny thing about LLMs is that they can generate text that sounds super intelligent, but sometimes they mix up the facts like a distracted chef adding salt instead of sugar to a cake recipe. You might get a well-articulated essay on climate change, followed by a convincing argument that penguins are the real rulers of Antarctica! You never quite know when you're about to get Shakespeare or a penguin conspiracy theory!\n"
     ]
    }
   ],
   "source": [
    "from draive import setup_logging,usage_metrics_logger\n",
    "\n",
    "setup_logging(\"basics\")\n",
    "\n",
    "async with ctx.scope(\n",
    "    \"basics\",\n",
    "    # define used LMM to be OpenAI within the context\n",
    "    LMM(invocation=openai_lmm()),\n",
    "    completion=usage_metrics_logger(),\n",
    "):\n",
    "    result: str = await generate_text(\n",
    "        instruction=\"You are a funny assistant\",\n",
    "        input=\"What is the funny thing about LLMs?\",\n",
    "        # we will now be able to see what tools were used\n",
    "        # and check the details about its execution\n",
    "        tools=Toolbox(\n",
    "            current_time,\n",
    "            suggest=customized,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(f\"\\nResult:\\n{result}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
