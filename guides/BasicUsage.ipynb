{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LLM usage\n",
    "\n",
    "Draive framework provides various ways to use LLM depending on the use case. The simplest interface is to generate text by using `generate_text` function. We can use it to make a simple text completion function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draive import generate_text\n",
    "\n",
    "\n",
    "async def text_completion(text: str) -> str:\n",
    "    # generate_text is a simple interface for generating text\n",
    "    return await generate_text(\n",
    "        # We have to provide instructions / system prompt to instruct the model\n",
    "        instruction=\"Prepare the simplest completion of a given text\",\n",
    "        # input is provided separately\n",
    "        input=text,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this function is a completion from a currently used model. What is a currently used model? We have to define it yet by providing basic setup of state and dependencies. In this example we are going to use OpenAI client, you have to provide the api key to that service in .env file with `OPENAI_API_KEY` key before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draive import load_env\n",
    "\n",
    "load_env()  # load .env variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have .env loaded we can prepare a context scope with OpenAI client and use our function. The lowest-level interface is called LMM. Draive supports multi-model solutions out of the box. Using the `openai_lmm` result as the current scope state selects this provider for our completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Violets are blue,  \n",
      "Sugar is sweet,  \n",
      "And so are you.\n"
     ]
    }
   ],
   "source": [
    "from draive import ctx\n",
    "from draive.openai import OpenAIChatConfig, openai_lmm\n",
    "\n",
    "async with ctx.scope(  # prepare new context\n",
    "    \"basics\",\n",
    "    openai_lmm(),  # set currently used LMM to OpenAI\n",
    "    OpenAIChatConfig(model=\"gpt-4o-mini\"), # select used model\n",
    "):\n",
    "    result: str = await text_completion(\n",
    "        text=\"Roses are red...\",\n",
    "    )\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we now know how to setup OpenAI as out LLM provider. We can start customizing it more by providing GPT model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT GPT 3.5 | temperature 0.4: Violets are blue.\n",
      "RESULT GPT 4o | temperature 0.4: Violets are blue.\n",
      "RESULT GPT 3.5 | temperature 0.7: Violets are blue.\n"
     ]
    }
   ],
   "source": [
    "from draive.openai import OpenAIChatConfig\n",
    "\n",
    "async with ctx.scope(  # prepare the new context\n",
    "    \"basics\",\n",
    "    openai_lmm(),\n",
    "    # define GPT model configuration as a context scope state\n",
    "    OpenAIChatConfig(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.4,\n",
    "    ),\n",
    "):\n",
    "    # now we are using gpt-3.5-turbo with temperature of 0.4\n",
    "    result: str = await text_completion(\n",
    "        text=\"Roses are red...\",\n",
    "    )\n",
    "\n",
    "    print(\"RESULT GPT 3.5 | temperature 0.4:\", result)\n",
    "\n",
    "    # we can update the configuration to change any parameter for nested context\n",
    "    with ctx.updated(\n",
    "        # we are updating the current context value instead of making a new one\n",
    "        # this allows to preserve other elements of the configuration\n",
    "        ctx.state(OpenAIChatConfig).updated(\n",
    "            model=\"gpt-4o\",\n",
    "        ),\n",
    "    ):\n",
    "        # now we are using gpt-4o with temperature of 0.4\n",
    "        result = await text_completion(\n",
    "            text=\"Roses are red...\",\n",
    "        )\n",
    "\n",
    "        print(\"RESULT GPT 4o | temperature 0.4:\", result)\n",
    "\n",
    "    # we can also update the configuration for a single call\n",
    "    # when using generate_text function directly\n",
    "    # here we are using gpt-3.5-turbo with temperature of 0.7\n",
    "    result = await generate_text(\n",
    "        instruction=\"Prepare simplest completion of given text\",\n",
    "        input=\"Roses are red...\",\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    print(\"RESULT GPT 3.5 | temperature 0.7:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know the basics, now we can examine the details of our execution to see what actually happened inside. We can setup the logger before execution and assign a logging metrics handler to see context metrics logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/Jan/2025:15:37:53 +0000 [DEBUG] [basics] [c3a16bf145604761ac2df6b8a8e86c01] [basics] [f6c17e1601424c34b069b75c9ae8dea5] Entering context...\n",
      "07/Jan/2025:15:37:54 +0000 [DEBUG] [basics] [c3a16bf145604761ac2df6b8a8e86c01] [basics] [f6c17e1601424c34b069b75c9ae8dea5] Metrics summary:\n",
      "⎡ @basics [f6c17e1601424c34b069b75c9ae8dea5](1.11s):\n",
      "|  \n",
      "|  ⎡ @generate_text [a8f8769d6d1d4d22ae156b2bc88a33cd](0.34s):\n",
      "|  |  \n",
      "|  |  ⎡ @openai_lmm_invocation [6ec28b7d12394caab3e4ec437742dc4e](0.34s):\n",
      "|  |  |  ⎡ •ArgumentsTrace:\n",
      "|  |  |  |  ├ kwargs: \n",
      "|  |  |  |  |  [instruction]: \"Prepare the simplest completion of a given text\"\n",
      "|  |  |  |  |  [context]: \n",
      "|  |  |  |  |  |  [0] content: \n",
      "|  |  |  |  |  |  |    parts: \n",
      "|  |  |  |  |  |  |      - text: Roses are red...\n",
      "|  |  |  |  |  |  |        meta: None\n",
      "|  |  |  |  |  [tool_selection]: \"auto\"\n",
      "|  |  |  |  |  [output]: \"text\"\n",
      "|  |  |  ⌊\n",
      "|  |  |  ⎡ •OpenAIChatConfig:\n",
      "|  |  |  |  ├ model: \"gpt-3.5-turbo\"\n",
      "|  |  |  |  ├ temperature: 0.4\n",
      "|  |  |  ⌊\n",
      "|  |  |  ⎡ •TokenUsage:\n",
      "|  |  |  |  ├ usage: \n",
      "|  |  |  |  |  [gpt-3.5-turbo]: \n",
      "|  |  |  |  |  ├ input_tokens: 24\n",
      "|  |  |  |  |  ├ output_tokens: 6\n",
      "|  |  |  ⌊\n",
      "|  |  |  ⎡ •ResultTrace:\n",
      "|  |  |  |  ├ result: \"Violets are blue\"\n",
      "|  |  |  ⌊\n",
      "|  |  ⌊\n",
      "|  ⌊\n",
      "|  \n",
      "|  ⎡ @generate_text [6e63452b80f5472fb2d084b8385c316b](0.76s):\n",
      "|  |  \n",
      "|  |  ⎡ @openai_lmm_invocation [1b00addcbe5f47ab8737f1457c79878f](0.76s):\n",
      "|  |  |  ⎡ •ArgumentsTrace:\n",
      "|  |  |  |  ├ kwargs: \n",
      "|  |  |  |  |  [instruction]: \"Prepare the simplest completion of a given text\"\n",
      "|  |  |  |  |  [context]: \n",
      "|  |  |  |  |  |  [0] content: \n",
      "|  |  |  |  |  |  |    parts: \n",
      "|  |  |  |  |  |  |      - text: Roses are red...\n",
      "|  |  |  |  |  |  |        meta: None\n",
      "|  |  |  |  |  [tool_selection]: \"auto\"\n",
      "|  |  |  |  |  [output]: \"text\"\n",
      "|  |  |  ⌊\n",
      "|  |  |  ⎡ •OpenAIChatConfig:\n",
      "|  |  |  |  ├ model: \"gpt-4o\"\n",
      "|  |  |  |  ├ temperature: 0.4\n",
      "|  |  |  ⌊\n",
      "|  |  |  ⎡ •TokenUsage:\n",
      "|  |  |  |  ├ usage: \n",
      "|  |  |  |  |  [gpt-4o]: \n",
      "|  |  |  |  |  ├ input_tokens: 24\n",
      "|  |  |  |  |  ├ output_tokens: 7\n",
      "|  |  |  ⌊\n",
      "|  |  |  ⎡ •OpenAISystemFingerprint:\n",
      "|  |  |  |  ├ system_fingerprint: \"fp_5f20662549\"\n",
      "|  |  |  ⌊\n",
      "|  |  |  ⎡ •ResultTrace:\n",
      "|  |  |  |  ├ result: \"Violets are blue.\"\n",
      "|  |  |  ⌊\n",
      "|  |  ⌊\n",
      "|  ⌊\n",
      "⌊\n",
      "07/Jan/2025:15:37:54 +0000 [DEBUG] [basics] [c3a16bf145604761ac2df6b8a8e86c01] [basics] [f6c17e1601424c34b069b75c9ae8dea5] ...exiting context after 1.11s\n"
     ]
    }
   ],
   "source": [
    "from draive import MetricsLogger, setup_logging\n",
    "\n",
    "setup_logging(\"basics\")  # setup logger\n",
    "\n",
    "async with ctx.scope(  # prepare the context and see the execution metrics report\n",
    "    \"basics\",\n",
    "    openai_lmm(),\n",
    "    OpenAIChatConfig(  # define GPT model configuration\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.4,\n",
    "    ),\n",
    "    metrics=MetricsLogger.handler()\n",
    "):\n",
    "    await text_completion(\n",
    "        text=\"Roses are red...\",\n",
    "    )\n",
    "\n",
    "    with ctx.updated(\n",
    "        ctx.state(OpenAIChatConfig).updated(\n",
    "            model=\"gpt-4o\",\n",
    "        ),\n",
    "    ):\n",
    "        await text_completion(\n",
    "            text=\"Roses are red...\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more advanced usage and use cases can be explored in other notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
