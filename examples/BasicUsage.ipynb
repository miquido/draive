{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LLM usage\n",
    "\n",
    "Draive framework provides various ways to use LLM depending on the use case. The simplest interface is to generate text by using `generate_text` function. We can use it to make a simple text completion function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draive import generate_text\n",
    "\n",
    "\n",
    "async def text_completion(text: str) -> str:\n",
    "    # generate_text is a simple interface for generating text\n",
    "    return await generate_text(\n",
    "        # We have to provide instructions / system prompt to instruct the model\n",
    "        instruction=\"Prepare the simplest completion of a given text\",\n",
    "        # input is provided separately\n",
    "        input=text,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this function is a completion from a currently used model. What is a currently used model? We have to define it yet by providing basic setup of state and dependencies. In this example we are going to use OpenAI client, you have to provide the api key to that service in .env file with `OPENAI_API_KEY` key before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draive import load_env\n",
    "\n",
    "load_env()  # load .env variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have .env loaded we can prepare a context scope with OpenAI client and use our function. Lowest level interface is called LMM since draive supports multi-model solutions out of the box. Assigning `invocation` to `openai_lmm_invocation` and using it as the context scope state selects this provider for our completions. We have also define OpenAI client dependency which defines functions for accessing OpenAI services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Violets are blue\n"
     ]
    }
   ],
   "source": [
    "from draive import LMM, OpenAIClient, ctx, openai_lmm_invocation\n",
    "\n",
    "async with ctx.new(  # prepare new context\n",
    "    \"text_completion\",\n",
    "    state=[LMM(invocation=openai_lmm_invocation)],  # set currently used LMM to OpenAI\n",
    "    dependencies=[OpenAIClient],  # use OpenAIClient dependency for accessing OpenAI services\n",
    "):\n",
    "    result: str = await text_completion(\n",
    "        text=\"Roses are red...\",\n",
    "    )\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we now know how to setup OpenAI as out LLM provider. We can start customizing it more by providing GPT model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT GPT 3.5 | temperature 0.4: Violets are blue.\n",
      "RESULT GPT 4o | temperature 0.4: Violets are blue,\n",
      "Sugar is sweet,\n",
      "And so are you.\n",
      "RESULT GPT 3.5 | temperature 0.7: Violets are blue.\n"
     ]
    }
   ],
   "source": [
    "from draive import OpenAIChatConfig\n",
    "\n",
    "async with ctx.new(  # prepare the new context\n",
    "    \"text_completion\",\n",
    "    state=[\n",
    "        LMM(invocation=openai_lmm_invocation),\n",
    "        # define GPT model configuration as a context scope state\n",
    "        OpenAIChatConfig(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0.4,\n",
    "        ),\n",
    "    ],\n",
    "    dependencies=[OpenAIClient],\n",
    "):\n",
    "    # now we are using gpt-3.5-turbo with temperature of 0.4\n",
    "    result: str = await text_completion(\n",
    "        text=\"Roses are red...\",\n",
    "    )\n",
    "\n",
    "    print(\"RESULT GPT 3.5 | temperature 0.4:\", result)\n",
    "\n",
    "    # we can update the configuration to change any parameter for nested context\n",
    "    with ctx.updated(\n",
    "        # we are updating the current context value instead of making a new one\n",
    "        # this allows to preserve other elements of the configuration\n",
    "        ctx.state(OpenAIChatConfig).updated(\n",
    "            model=\"gpt-4o\",\n",
    "        ),\n",
    "    ):\n",
    "        # now we are using gpt-4o with temperature of 0.4\n",
    "        result = await text_completion(\n",
    "            text=\"Roses are red...\",\n",
    "        )\n",
    "\n",
    "        print(\"RESULT GPT 4o | temperature 0.4:\", result)\n",
    "\n",
    "    # we can also update the configuration for a single call\n",
    "    # when using generate_text function directly\n",
    "    # here we are using gpt-3.5-turbo with temperature of 0.7\n",
    "    result = await generate_text(\n",
    "        instruction=\"Prepare simplest completion of given text\",\n",
    "        input=\"Roses are red...\",\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    print(\"RESULT GPT 3.5 | temperature 0.7:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we knows the basics, now we can examine the details of our execution to see what actually happened inside. We can setup the logger before execution to see context metrics logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/Jun/2024:12:31:12 +0000 [INFO] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|text_completion] started...\n",
      "07/Jun/2024:12:31:12 +0000 [INFO] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|lmm_generate_text] started...\n",
      "07/Jun/2024:12:31:12 +0000 [INFO] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|openai_lmm_completion] started...\n",
      "07/Jun/2024:12:31:12 +0000 [DEBUG] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|openai_lmm_completion] Requested OpenAI lmm\n",
      "07/Jun/2024:12:31:13 +0000 [INFO] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|openai_lmm_completion] ...finished after 0.62s\n",
      "07/Jun/2024:12:31:13 +0000 [DEBUG] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|lmm_generate_text] Received text generation result\n",
      "07/Jun/2024:12:31:13 +0000 [INFO] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|lmm_generate_text] ...finished after 0.62s\n",
      "07/Jun/2024:12:31:13 +0000 [INFO] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|lmm_generate_text] started...\n",
      "07/Jun/2024:12:31:13 +0000 [INFO] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|openai_lmm_completion] started...\n",
      "07/Jun/2024:12:31:13 +0000 [DEBUG] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|openai_lmm_completion] Requested OpenAI lmm\n",
      "07/Jun/2024:12:31:14 +0000 [INFO] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|openai_lmm_completion] ...finished after 1.56s\n",
      "07/Jun/2024:12:31:14 +0000 [DEBUG] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|lmm_generate_text] Received text generation result\n",
      "07/Jun/2024:12:31:14 +0000 [INFO] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|lmm_generate_text] ...finished after 1.56s\n",
      "07/Jun/2024:12:31:14 +0000 [INFO] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|lmm_generate_text] started...\n",
      "07/Jun/2024:12:31:14 +0000 [INFO] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|openai_lmm_completion] started...\n",
      "07/Jun/2024:12:31:14 +0000 [DEBUG] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|openai_lmm_completion] Requested OpenAI lmm\n",
      "07/Jun/2024:12:31:15 +0000 [INFO] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|openai_lmm_completion] ...finished after 0.50s\n",
      "07/Jun/2024:12:31:15 +0000 [DEBUG] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|lmm_generate_text] Received text generation result\n",
      "07/Jun/2024:12:31:15 +0000 [INFO] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|lmm_generate_text] ...finished after 0.50s\n",
      "07/Jun/2024:12:31:15 +0000 [INFO] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1|text_completion] ...finished after 2.68s\n",
      "07/Jun/2024:12:31:15 +0000 [INFO] [text_completion] [53d719cf34ec44c9bb7c9c2c73ac97a1] Metrics report:\n",
      "@text_completion(2.68s):\n",
      "• TokenUsage:\n",
      "|  + usage: \n",
      "|  |  + gpt-3.5-turbo: \n",
      "|  |  |  + input_tokens: 46\n",
      "|  |  |  + output_tokens: 12\n",
      "|  |  + gpt-4o: \n",
      "|  |  |  + input_tokens: 24\n",
      "|  |  |  + output_tokens: 15\n",
      "@lmm_generate_text(0.62s):\n",
      "|  • TokenUsage:\n",
      "|  |  + usage: \n",
      "|  |  |  + gpt-3.5-turbo: \n",
      "|  |  |  |  + input_tokens: 24\n",
      "|  |  |  |  + output_tokens: 6\n",
      "|  |  |  + gpt-4o: \n",
      "|  |  |  |  + input_tokens: 24\n",
      "|  |  |  |  + output_tokens: 15\n",
      "|  @openai_lmm_completion(0.62s):\n",
      "|  |  • ArgumentsTrace:\n",
      "|  |  |  + kwargs: \n",
      "|  |  |  |  + context: \n",
      "|  |  |  |  |  [0] \n",
      "|  |  |  |  |  |  + content: Prepare the simplest completion of a given text\n",
      "|  |  |  |  |  [1] \n",
      "|  |  |  |  |  |  + content: \n",
      "|  |  |  |  |  |  |  + elements: ('Roses are red...',)\n",
      "|  |  |  |  + require_tool: False\n",
      "|  |  |  |  + output: text\n",
      "|  |  |  |  + stream: False\n",
      "|  |  • OpenAIChatConfig:\n",
      "|  |  |  + model: gpt-3.5-turbo\n",
      "|  |  |  + temperature: 0.4\n",
      "|  |  • TokenUsage:\n",
      "|  |  |  + usage: \n",
      "|  |  |  |  + gpt-3.5-turbo: \n",
      "|  |  |  |  |  + input_tokens: 24\n",
      "|  |  |  |  |  + output_tokens: 6\n",
      "|  |  |  |  + gpt-4o: \n",
      "|  |  |  |  |  + input_tokens: 24\n",
      "|  |  |  |  |  + output_tokens: 15\n",
      "|  |  • ResultTrace:\n",
      "|  |  |  + result: Violets are blue.\n",
      "@lmm_generate_text(1.56s):\n",
      "|  • TokenUsage:\n",
      "|  |  + usage: \n",
      "|  |  |  + gpt-4o: \n",
      "|  |  |  |  + input_tokens: 24\n",
      "|  |  |  |  + output_tokens: 15\n",
      "|  @openai_lmm_completion(1.56s):\n",
      "|  |  • ArgumentsTrace:\n",
      "|  |  |  + kwargs: \n",
      "|  |  |  |  + context: \n",
      "|  |  |  |  |  [0] \n",
      "|  |  |  |  |  |  + content: Prepare the simplest completion of a given text\n",
      "|  |  |  |  |  [1] \n",
      "|  |  |  |  |  |  + content: \n",
      "|  |  |  |  |  |  |  + elements: ('Roses are red...',)\n",
      "|  |  |  |  + require_tool: False\n",
      "|  |  |  |  + output: text\n",
      "|  |  |  |  + stream: False\n",
      "|  |  • OpenAIChatConfig:\n",
      "|  |  |  + model: gpt-4o\n",
      "|  |  |  + temperature: 0.4\n",
      "|  |  • TokenUsage:\n",
      "|  |  |  + usage: \n",
      "|  |  |  |  + gpt-4o: \n",
      "|  |  |  |  |  + input_tokens: 24\n",
      "|  |  |  |  |  + output_tokens: 15\n",
      "|  |  • OpenAISystemFingerprint:\n",
      "|  |  |  + system_fingerprint: fp_aa87380ac5\n",
      "|  |  • ResultTrace:\n",
      "|  |  |  + result: Violets are blue,\n",
      "|  |  |  Sugar is sweet,\n",
      "|  |  |  And so are you.\n",
      "@lmm_generate_text(0.50s):\n",
      "|  • TokenUsage:\n",
      "|  |  + usage: \n",
      "|  |  |  + gpt-3.5-turbo: \n",
      "|  |  |  |  + input_tokens: 22\n",
      "|  |  |  |  + output_tokens: 6\n",
      "|  @openai_lmm_completion(0.50s):\n",
      "|  |  • ArgumentsTrace:\n",
      "|  |  |  + kwargs: \n",
      "|  |  |  |  + context: \n",
      "|  |  |  |  |  [0] \n",
      "|  |  |  |  |  |  + content: Prepare simplest completion of given text\n",
      "|  |  |  |  |  [1] \n",
      "|  |  |  |  |  |  + content: \n",
      "|  |  |  |  |  |  |  + elements: ('Roses are red...',)\n",
      "|  |  |  |  + require_tool: False\n",
      "|  |  |  |  + output: text\n",
      "|  |  |  |  + stream: False\n",
      "|  |  |  |  + temperature: 0.7\n",
      "|  |  • OpenAIChatConfig:\n",
      "|  |  |  + model: gpt-3.5-turbo\n",
      "|  |  |  + temperature: 0.7\n",
      "|  |  • TokenUsage:\n",
      "|  |  |  + usage: \n",
      "|  |  |  |  + gpt-3.5-turbo: \n",
      "|  |  |  |  |  + input_tokens: 22\n",
      "|  |  |  |  |  + output_tokens: 6\n",
      "|  |  • ResultTrace:\n",
      "|  |  |  + result: Violets are blue.\n"
     ]
    }
   ],
   "source": [
    "from draive import setup_logging\n",
    "\n",
    "setup_logging(\"text_completion\")  # setup logger\n",
    "\n",
    "async with ctx.new(  # prepare the context and see the execution metrics report\n",
    "    \"text_completion\",\n",
    "    state=[\n",
    "        LMM(invocation=openai_lmm_invocation),\n",
    "        OpenAIChatConfig(  # define GPT model configuration\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0.4,\n",
    "        ),\n",
    "    ],\n",
    "    dependencies=[OpenAIClient],\n",
    "):\n",
    "    await text_completion(\n",
    "        text=\"Roses are red...\",\n",
    "    )\n",
    "\n",
    "    with ctx.updated(\n",
    "        ctx.state(OpenAIChatConfig).updated(\n",
    "            model=\"gpt-4o\",\n",
    "        ),\n",
    "    ):\n",
    "        await text_completion(\n",
    "            text=\"Roses are red...\",\n",
    "        )\n",
    "\n",
    "    await generate_text(\n",
    "        instruction=\"Prepare simplest completion of given text\",\n",
    "        input=\"Roses are red...\",\n",
    "        temperature=0.7,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more advanced usage and use cases can be explored in other notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
