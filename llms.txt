# Draive + Haiway LLM Integration Cheat Sheet

Authoritative, concise runtime notes for wiring Draive apps. Targets Python 3.13+, strict typing, async-only I/O.

## Setup
- Install: `pip install draive` (or uv/pip with extras). Common extras: `draive[openai]`, `draive[openai_realtime]`, `draive[anthropic]`, `draive[mistral]`, `draive[gemini]`, `draive[cohere]`, `draive[vllm]`, `draive[ollama]`, `draive[qdrant]`, `draive[postgres]`.
- Environment: store provider keys in env or `.env`. Call `load_env()` once at startup; Draive never auto-loads secrets.
```python
from draive import load_env
load_env()
```

## Core primitives
- `State` (from haiway): immutable runtime config/resources. Update via `.updating(...)`.
- `ctx.scope(label, *state, disposables=(...), observability=...)`: structured async context that activates states and disposables; nests safely.
- `Disposables`: async resources (e.g., `OpenAI()`, `QdrantClient()`) auto-cleaned on scope exit.
- `DataModel`: immutable, validated data containers with `.json_schema()` / `.simplified_schema()` and `.to_str()`.
- `MultimodalContent`: ordered parts (`TextContent`, `ResourceContent`/`ResourceReference`, `ArtifactContent`, `MultimodalTag`). Use `MultimodalContent.of(...)`; `.to_str()` redacts binary by default.

## Provider wiring (OpenAI example)
```python
import asyncio
from draive import ctx, TextGeneration
from draive.openai import OpenAI, OpenAIResponsesConfig

async def main() -> None:
    async with ctx.scope(
        "quickstart",
        OpenAIResponsesConfig(model="gpt-5-mini"),
        disposables=(OpenAI(),),
    ):
        reply = await TextGeneration.generate(
            instructions="You are concise.",
            input="List three benefits of typed prompts.",
            temperature=0.2,
        )
        ctx.log_info("reply", content=reply)

asyncio.run(main())
```
*Models*: `OpenAIResponsesConfig.model` literals: `gpt-5`, `gpt-5-mini`, `gpt-5-nano` (or custom string). Key knobs: `temperature`, `parallel_tool_calls` (bool), `max_output_tokens`, `service_tier`, `truncation`.

## Structured generation
Use `ModelGeneration.generate` with a `DataModel`; optional `schema_injection={"full","simplified","skip"}` (default `"skip"`).
```python
from collections.abc import Sequence
from draive import DataModel, ModelGeneration
from draive.openai import OpenAIResponsesConfig, OpenAI

class Summary(DataModel):
    topic: str
    bullets: Sequence[str]

async with ctx.scope(
    "structured",
    OpenAIResponsesConfig(model="gpt-5-mini"),
    disposables=(OpenAI(),),
):
    summary = await ModelGeneration.generate(
        Summary,
        instructions="Summarize clearly in bullets.",
        input="Typed prompts improve clarity and tooling.",
        schema_injection="simplified",
    )
```

## Tool calling
Decorate async functions with `@tool`; pass to generation APIs via list or `Toolbox`.
```python
from draive import tool, TextGeneration
from draive.openai import OpenAI, OpenAIResponsesConfig

@tool(description="Return UTC timestamp")
async def now_utc() -> str:
    from datetime import datetime, UTC
    return datetime.now(UTC).isoformat()

async with ctx.scope(
    "tools",
    OpenAIResponsesConfig(model="gpt-5-mini"),
    disposables=(OpenAI(),),
):
    result = await TextGeneration.generate(
        instructions="Use tools when helpful.",
        input="What time is it?",
        tools=[now_utc],
    )
```

## Conversations
`Conversation.completion` handles chat-style turns; accepts tools and memory.
```python
from draive import Conversation
from draive.models import ModelMemory
from draive.openai import OpenAI, OpenAIResponsesConfig

memory = ModelMemory.constant()  # static or replace with persistent implementation

async with ctx.scope(
    "chat",
    OpenAIResponsesConfig(model="gpt-5-mini"),
    memory,
    disposables=(OpenAI(),),
):
    msg = await Conversation.completion(
        instructions="Be brief.",
        input="Hello!",
    )
    text_only = msg.content.to_str()
```

## Multimodal prompts
```python
from draive import TextGeneration
from draive.multimodal import MultimodalContent, ResourceContent, TextContent

prompt = MultimodalContent.of(
    TextContent.of("Describe this image"),
    ResourceContent.of(image_bytes, mime_type="image/png"),
)
await TextGeneration.generate(
    instructions="Respond in one sentence.",
    input=prompt,
)
```

## Embeddings & retrieval
- `VectorIndex` state abstracts indexing/search; provide embedding config + client in scope.
- `draive.helpers.VolatileVectorIndex()` gives in-memory index for tests.
```python
from draive import VectorIndex
from draive.helpers import VolatileVectorIndex
from draive.openai import OpenAI, OpenAIEmbeddingConfig
from draive.parameters import DataModel

class Chunk(DataModel):
    id: str
    content: str

index = VolatileVectorIndex()
async with ctx.scope(
    "rag",
    index,
    OpenAIEmbeddingConfig(model="text-embedding-3-small"),
    disposables=(OpenAI(),),
):
    await index.index(Chunk, values=[Chunk(id="1", content="hello world")], attribute=Chunk._.content)
    hits = await index.search(Chunk, query="hello", limit=3)
```

## Guardrails
Attach guardrail states to a scope; exceptions raised on violations.
```python
from draive.guardrails import GuardrailsModeration

async with ctx.scope("moderated", GuardrailsModeration.of(provider="openai")):
    ...
```
Other guardrails: privacy, safety, quality; all live under `draive.guardrails`.

## Evaluation
Use `@evaluator` to score outputs; returned `EvaluatorResult` exposes `.passed` and `.performance` (%).
```python
from draive.evaluation import evaluator, evaluate
from draive.evaluators import readability_evaluator

@evaluator(name="keyword", threshold=0.8)
async def keyword_eval(content: str, required: list[str]) -> float:
    return sum(k in content for k in required) / len(required)

results = await evaluate(
    "AI improves productivity",
    readability_evaluator.prepared(),
    keyword_eval.prepared(required=["AI", "productivity"]),
)
all_ok = all(r.passed for r in results)
```

## Observability
- Prefer `ctx.log_debug/info/warn/error` over `print`.
- Emit metrics via `ctx.record_info(metric, value=..., attributes=...)`.
- Use meaningful `ctx.scope` labels; they appear in traces/metrics.

## Error handling
- Keep boundaries strict: catch provider SDK errors and rethrow typed exceptions with redacted context.
- Never log secrets or raw payloads; rely on env vars (`getenv_str`, etc.).

## Execution rules (apply everywhere)
- Always run inside `ctx.scope`.
- Keep public APIs strictly typed (no implicit `Any`).
- Avoid blocking calls; everything I/O is async.
- Mutate state via `.updating(...)`; states and data models are immutable by design.
- Use `MultimodalContent` for any model input/output that mixes text/media/tools; use `.to_str()` for safe logging (binary redacted).
